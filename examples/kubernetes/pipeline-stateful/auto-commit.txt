Step 1/6 : FROM striot/striot-base:latest
 ---> 2a98f6ca4956
Step 2/6 : WORKDIR /opt/node
 ---> Running in 2b6a23e2d84f
Removing intermediate container 2b6a23e2d84f
 ---> 24553543bd1e
Step 3/6 : COPY . /opt/node
 ---> ed179bc3a03d
Step 4/6 : RUN ghc node.hs
 ---> Running in 23a3151060ba
[1 of 1] Compiling Main             ( node.hs, node.o )
Linking node ...
Removing intermediate container 23a3151060ba
 ---> df81ff0b3510
Step 5/6 : EXPOSE 9001
 ---> Running in e39ab14b3461
Removing intermediate container e39ab14b3461
 ---> 573ce95cfbbe
Step 6/6 : CMD /opt/node/node
 ---> Running in 4ccd03cd9418
Removing intermediate container 4ccd03cd9418
 ---> ab9c37a34563
Successfully built ab9c37a34563
Successfully tagged pipeline-stateful_node3:latest
Step 1/5 : FROM striot/striot-base:latest
 ---> 2a98f6ca4956
Step 2/5 : WORKDIR /opt/node
 ---> Using cache
 ---> 24553543bd1e
Step 3/5 : COPY . /opt/node
 ---> 2989f458a104
Step 4/5 : RUN ghc node.hs
 ---> Running in 6701b87fe1af
[1 of 1] Compiling Main             ( node.hs, node.o )
Linking node ...
Removing intermediate container 6701b87fe1af
 ---> d04b9d4381ec
Step 5/5 : CMD /opt/node/node
 ---> Running in 523eef4b1436
Removing intermediate container 523eef4b1436
 ---> 4ac118d89434
Successfully built 4ac118d89434
Successfully tagged pipeline-stateful_node2:latest
Step 1/5 : FROM striot/striot-base:latest
 ---> 2a98f6ca4956
Step 2/5 : WORKDIR /opt/node
 ---> Using cache
 ---> 24553543bd1e
Step 3/5 : COPY . /opt/node
 ---> 2c10d83083b8
Step 4/5 : RUN ghc node.hs
 ---> Running in 6edbdcbee5aa
[1 of 1] Compiling Main             ( node.hs, node.o )
Linking node ...
Removing intermediate container 6edbdcbee5aa
 ---> ef5ecd91a943
Step 5/5 : CMD /opt/node/node
 ---> Running in 68d5edbde674
Removing intermediate container 68d5edbde674
 ---> 2b23204fa57f
Successfully built 2b23204fa57f
Successfully tagged pipeline-stateful_node1:latest
Step 1/5 : FROM striot/striot-base:latest
 ---> 2a98f6ca4956
Step 2/5 : WORKDIR /opt/node
 ---> Using cache
 ---> 24553543bd1e
Step 3/5 : COPY . /opt/node
 ---> 151e68555cde
Step 4/5 : RUN ghc node.hs
 ---> Running in f56c713b989a
[1 of 1] Compiling Main             ( node.hs, node.o )
Linking node ...
Removing intermediate container f56c713b989a
 ---> 81eda3076851
Step 5/5 : CMD /opt/node/node
 ---> Running in fce2943020c5
Removing intermediate container fce2943020c5
 ---> 26064af3798c
Successfully built 26064af3798c
Successfully tagged pipeline-stateful_manage-sender:latest
Attaching to pipeline-stateful_zookeeper_1, pipeline-stateful_kafka_1, pipeline-stateful_node3_1, pipeline-stateful_node2_1, pipeline-stateful_node1_1, pipeline-stateful_manage-sender_1
[36mkafka_1          |[0m waiting for kafka to be ready
[36mkafka_1          |[0m Excluding KAFKA_HOME from broker config
[32mnode1_1          |[0m "create new producer"
[32mnode1_1          |[0m "runhandler producer"
[32mnode1_1          |[0m %3|1586801637.254|FAIL|rdkafka#producer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connect to ipv4#192.168.64.4:9092 failed: Connection refused (after 0ms in state CONNECT)
[32mnode1_1          |[0m %3|1586801637.254|ERROR|rdkafka#producer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connect to ipv4#192.168.64.4:9092 failed: Connection refused (after 0ms in state CONNECT)
[32mnode1_1          |[0m %3|1586801637.254|ERROR|rdkafka#producer-1| [thrd:kafka:9092/bootstrap]: 1/1 brokers are down
[36mkafka_1          |[0m [Configuring] 'advertised.host.name' in '/opt/kafka/config/server.properties'
[35mnode2_1          |[0m "create new consumer"
[35mnode2_1          |[0m %7|1586801636.541|MEMBERID|rdkafka#consumer-1| [thrd:app]: Group "striot_con_group": updating member id "(not-set)" -> ""
[35mnode2_1          |[0m %7|1586801636.541|WAKEUPFD|rdkafka#consumer-1| [thrd:app]: GroupCoordinator: Enabled low-latency ops queue wake-ups
[35mnode2_1          |[0m %7|1586801636.541|BROKER|rdkafka#consumer-1| [thrd:app]: GroupCoordinator: Added new broker with NodeId -1
[35mnode2_1          |[0m %7|1586801636.541|BRKMAIN|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: Enter main broker thread
[35mnode2_1          |[0m %7|1586801636.541|BRKMAIN|rdkafka#consumer-1| [thrd::0/internal]: :0/internal: Enter main broker thread
[35mnode2_1          |[0m %7|1586801636.541|WAKEUPFD|rdkafka#consumer-1| [thrd:app]: kafka:9092/bootstrap: Enabled low-latency ops queue wake-ups
[35mnode2_1          |[0m %7|1586801636.541|BROKER|rdkafka#consumer-1| [thrd:app]: kafka:9092/bootstrap: Added new broker with NodeId -1
[35mnode2_1          |[0m %7|1586801636.541|INIT|rdkafka#consumer-1| [thrd:app]: librdkafka v1.4.0-RC4-selfstatic-test18 (0x1040005) rdkafka#consumer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4,sasl_gssapi,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer, GCC GXX PKGCONFIG INSTALL GNULD LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD HDRHISTOGRAM SYSLOG SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW, debug 0xfffff)
[35mnode2_1          |[0m %7|1586801636.541|BRKMAIN|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enter main broker thread
[35mnode2_1          |[0m %7|1586801636.541|CGRPOP|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" received op SUBSCRIBE (v0) in state init (join state init, v1 vs 0)
[35mnode2_1          |[0m %7|1586801636.541|SUBSCRIBE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": subscribe to new subscription of 1 topics (join state init)
[35mnode2_1          |[0m %7|1586801636.541|UNSUBSCRIBE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": unsubscribe from current unset subscription of 0 topics (leave group=no, join state init, v1)
[35mnode2_1          |[0m %7|1586801636.541|GRPLEADER|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": resetting group leader info: unsubscribe
[35mnode2_1          |[0m %7|1586801636.541|REBALANCE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" is rebalancing in state init (join-state init) without assignment: unsubscribe
[35mnode2_1          |[0m %7|1586801636.541|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state init -> wait-unassign (v1, state init)
[35mnode2_1          |[0m %7|1586801636.541|UNASSIGN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": unassign done in state init (join state wait-unassign): without new assignment: unassign (no previous assignment)
[35mnode2_1          |[0m %7|1586801636.541|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state wait-unassign -> init (v1, state init)
[35mnode2_1          |[0m %7|1586801636.541|CGRPSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed state init -> query-coord (v1, join-state init)
[35mnode2_1          |[0m %7|1586801636.541|BROADCAST|rdkafka#consumer-1| [thrd:main]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801636.541|CONNECT|rdkafka#consumer-1| [thrd:main]: kafka:9092/bootstrap: Selected for cluster connection: coordinator query (broker has 0 connection attempt(s))
[35mnode2_1          |[0m "runhandler consumer"
[35mnode2_1          |[0m "{1}: Retrieve Input"
[35mnode2_1          |[0m %7|1586801636.541|CGRPQUERY|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": no broker available for coordinator query: intervaled in state query-coord
[35mnode2_1          |[0m "{2}: Perform streamOp"
[35mnode2_1          |[0m %7|1586801636.541|CONNECT|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Received CONNECT op
[35mnode2_1          |[0m %7|1586801636.541|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT
[35mnode2_1          |[0m %7|1586801636.541|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801636.541|CONNECT|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: broker in state TRY_CONNECT connecting
[35mnode2_1          |[0m %7|1586801636.541|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT
[35mnode2_1          |[0m %7|1586801636.541|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[36;1mzookeeper_1      |[0m ZooKeeper JMX enabled by default
[35mnode2_1          |[0m %7|1586801636.542|CONNECT|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connecting to ipv4#192.168.64.4:9092 (plaintext) with socket 10
[35mnode2_1          |[0m %7|1586801636.542|BROKERFAIL|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection refused)
[35mnode2_1          |[0m %3|1586801636.542|FAIL|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connect to ipv4#192.168.64.4:9092 failed: Connection refused (after 0ms in state CONNECT)
[35mnode2_1          |[0m %3|1586801636.542|ERROR|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connect to ipv4#192.168.64.4:9092 failed: Connection refused (after 0ms in state CONNECT)
[35mnode2_1          |[0m %7|1586801636.542|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state CONNECT -> DOWN
[35mnode2_1          |[0m %3|1586801636.542|ERROR|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: 1/1 brokers are down
[35mnode2_1          |[0m %7|1586801636.542|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801636.542|BUFQ|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Purging bufq with 0 buffers
[35mnode2_1          |[0m %7|1586801636.542|BUFQ|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Purging bufq with 0 buffers
[35mnode2_1          |[0m %7|1586801636.542|BUFQ|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Updating 0 buffers on connection reset
[35mnode2_1          |[0m %7|1586801636.542|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state DOWN -> INIT
[35mnode2_1          |[0m %7|1586801636.542|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801637.540|CONNECT|rdkafka#consumer-1| [thrd:main]: kafka:9092/bootstrap: Selected for cluster connection: coordinator query (broker has 1 connection attempt(s))
[35mnode2_1          |[0m %7|1586801637.540|CGRPQUERY|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": no broker available for coordinator query: intervaled in state query-coord
[35mnode2_1          |[0m %7|1586801637.540|CONNECT|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Received CONNECT op
[35mnode2_1          |[0m %7|1586801637.540|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT
[35mnode2_1          |[0m %7|1586801637.540|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801637.540|CONNECT|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: broker in state TRY_CONNECT connecting
[35mnode2_1          |[0m %7|1586801637.540|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT
[35mnode2_1          |[0m %7|1586801637.540|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801637.541|CONNECT|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connecting to ipv4#192.168.64.4:9092 (plaintext) with socket 10
[35mnode2_1          |[0m %7|1586801637.541|BROKERFAIL|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection refused)
[35mnode2_1          |[0m %7|1586801637.541|FAIL|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connect to ipv4#192.168.64.4:9092 failed: Connection refused (after 0ms in state CONNECT)
[35mnode2_1          |[0m %7|1586801637.541|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state CONNECT -> DOWN
[35mnode2_1          |[0m %7|1586801637.541|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801637.541|BUFQ|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Purging bufq with 0 buffers
[35mnode2_1          |[0m %7|1586801637.541|BUFQ|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Purging bufq with 0 buffers
[35mnode2_1          |[0m %7|1586801637.541|BUFQ|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Updating 0 buffers on connection reset
[35mnode2_1          |[0m %7|1586801637.541|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state DOWN -> INIT
[35mnode2_1          |[0m %7|1586801637.541|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[36mkafka_1          |[0m [Configuring] 'port' in '/opt/kafka/config/server.properties'
[36mkafka_1          |[0m [Configuring] 'broker.id' in '/opt/kafka/config/server.properties'
[35mnode2_1          |[0m %7|1586801637.541|CONNECT|rdkafka#consumer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection
[36;1mzookeeper_1      |[0m Using config: /opt/zookeeper-3.4.13/bin/../conf/zoo.cfg
[36mkafka_1          |[0m Excluding KAFKA_VERSION from broker config
[36mkafka_1          |[0m [Configuring] 'zookeeper.connect' in '/opt/kafka/config/server.properties'
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:55,993 [myid:] - INFO  [main:QuorumPeerConfig@136] - Reading configuration from: /opt/zookeeper-3.4.13/bin/../conf/zoo.cfg
[36mkafka_1          |[0m [Configuring] 'log.dirs' in '/opt/kafka/config/server.properties'
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:55,998 [myid:] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
[36mkafka_1          |[0m [2020-04-13 18:13:56,649] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:55,998 [myid:] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 1
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:55,998 [myid:] - WARN  [main:QuorumPeerMain@116] - Either no config or no quorum defined in config, running  in standalone mode
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:55,998 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[36mkafka_1          |[0m [2020-04-13 18:13:57,194] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,008 [myid:] - INFO  [main:QuorumPeerConfig@136] - Reading configuration from: /opt/zookeeper-3.4.13/bin/../conf/zoo.cfg
[36mkafka_1          |[0m [2020-04-13 18:13:57,194] INFO starting (kafka.server.KafkaServer)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,009 [myid:] - INFO  [main:ZooKeeperServerMain@98] - Starting server
[36mkafka_1          |[0m [2020-04-13 18:13:57,195] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
[36mkafka_1          |[0m [2020-04-13 18:13:57,210] INFO [ZooKeeperClient Kafka server] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,011 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[36mkafka_1          |[0m [2020-04-13 18:13:57,214] INFO Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT (org.apache.zookeeper.ZooKeeper)
[36mkafka_1          |[0m [2020-04-13 18:13:57,214] INFO Client environment:host.name=8d177c875ca7 (org.apache.zookeeper.ZooKeeper)
[36mkafka_1          |[0m [2020-04-13 18:13:57,214] INFO Client environment:java.version=1.8.0_212 (org.apache.zookeeper.ZooKeeper)
[36mkafka_1          |[0m [2020-04-13 18:13:57,214] INFO Client environment:java.vendor=IcedTea (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,015 [myid:] - INFO  [main:Environment@100] - Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 04:05 GMT
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,015 [myid:] - INFO  [main:Environment@100] - Server environment:host.name=7f3d0233ecd7
[36mkafka_1          |[0m [2020-04-13 18:13:57,214] INFO Client environment:java.home=/usr/lib/jvm/java-1.8-openjdk/jre (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,015 [myid:] - INFO  [main:Environment@100] - Server environment:java.version=1.7.0_65
[36mkafka_1          |[0m [2020-04-13 18:13:57,214] INFO Client environment:java.class.path=/opt/kafka/bin/../libs/activation-1.1.1.jar:/opt/kafka/bin/../libs/aopalliance-repackaged-2.5.0.jar:/opt/kafka/bin/../libs/argparse4j-0.7.0.jar:/opt/kafka/bin/../libs/audience-annotations-0.5.0.jar:/opt/kafka/bin/../libs/commons-cli-1.4.jar:/opt/kafka/bin/../libs/commons-lang3-3.8.1.jar:/opt/kafka/bin/../libs/connect-api-2.4.1.jar:/opt/kafka/bin/../libs/connect-basic-auth-extension-2.4.1.jar:/opt/kafka/bin/../libs/connect-file-2.4.1.jar:/opt/kafka/bin/../libs/connect-json-2.4.1.jar:/opt/kafka/bin/../libs/connect-mirror-2.4.1.jar:/opt/kafka/bin/../libs/connect-mirror-client-2.4.1.jar:/opt/kafka/bin/../libs/connect-runtime-2.4.1.jar:/opt/kafka/bin/../libs/connect-transforms-2.4.1.jar:/opt/kafka/bin/../libs/guava-20.0.jar:/opt/kafka/bin/../libs/hk2-api-2.5.0.jar:/opt/kafka/bin/../libs/hk2-locator-2.5.0.jar:/opt/kafka/bin/../libs/hk2-utils-2.5.0.jar:/opt/kafka/bin/../libs/jackson-annotations-2.10.0.jar:/opt/kafka/bin/../libs/jackson-core-2.10.0.jar:/opt/kafka/bin/../libs/jackson-databind-2.10.0.jar:/opt/kafka/bin/../libs/jackson-dataformat-csv-2.10.0.jar:/opt/kafka/bin/../libs/jackson-datatype-jdk8-2.10.0.jar:/opt/kafka/bin/../libs/jackson-jaxrs-base-2.10.0.jar:/opt/kafka/bin/../libs/jackson-jaxrs-json-provider-2.10.0.jar:/opt/kafka/bin/../libs/jackson-module-jaxb-annotations-2.10.0.jar:/opt/kafka/bin/../libs/jackson-module-paranamer-2.10.0.jar:/opt/kafka/bin/../libs/jackson-module-scala_2.12-2.10.0.jar:/opt/kafka/bin/../libs/jakarta.activation-api-1.2.1.jar:/opt/kafka/bin/../libs/jakarta.annotation-api-1.3.4.jar:/opt/kafka/bin/../libs/jakarta.inject-2.5.0.jar:/opt/kafka/bin/../libs/jakarta.ws.rs-api-2.1.5.jar:/opt/kafka/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/opt/kafka/bin/../libs/javassist-3.22.0-CR2.jar:/opt/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/opt/kafka/bin/../libs/javax.ws.rs-api-2.1.1.jar:/opt/kafka/bin/../libs/jaxb-api-2.3.0.jar:/opt/kafka/bin/../libs/jersey-client-2.28.jar:/opt/kafka/bin/../libs/jersey-common-2.28.jar:/opt/kafka/bin/../libs/jersey-container-servlet-2.28.jar:/opt/kafka/bin/../libs/jersey-container-servlet-core-2.28.jar:/opt/kafka/bin/../libs/jersey-hk2-2.28.jar:/opt/kafka/bin/../libs/jersey-media-jaxb-2.28.jar:/opt/kafka/bin/../libs/jersey-server-2.28.jar:/opt/kafka/bin/../libs/jetty-client-9.4.20.v20190813.jar:/opt/kafka/bin/../libs/jetty-continuation-9.4.20.v20190813.jar:/opt/kafka/bin/../libs/jetty-http-9.4.20.v20190813.jar:/opt/kafka/bin/../libs/jetty-io-9.4.20.v20190813.jar:/opt/kafka/bin/../libs/jetty-security-9.4.20.v20190813.jar:/opt/kafka/bin/../libs/jetty-server-9.4.20.v20190813.jar:/opt/kafka/bin/../libs/jetty-servlet-9.4.20.v20190813.jar:/opt/kafka/bin/../libs/jetty-servlets-9.4.20.v20190813.jar:/opt/kafka/bin/../libs/jetty-util-9.4.20.v20190813.jar:/opt/kafka/bin/../libs/jopt-simple-5.0.4.jar:/opt/kafka/bin/../libs/kafka-clients-2.4.1.jar:/opt/kafka/bin/../libs/kafka-log4j-appender-2.4.1.jar:/opt/kafka/bin/../libs/kafka-streams-2.4.1.jar:/opt/kafka/bin/../libs/kafka-streams-examples-2.4.1.jar:/opt/kafka/bin/../libs/kafka-streams-scala_2.12-2.4.1.jar:/opt/kafka/bin/../libs/kafka-streams-test-utils-2.4.1.jar:/opt/kafka/bin/../libs/kafka-tools-2.4.1.jar:/opt/kafka/bin/../libs/kafka_2.12-2.4.1-sources.jar:/opt/kafka/bin/../libs/kafka_2.12-2.4.1.jar:/opt/kafka/bin/../libs/log4j-1.2.17.jar:/opt/kafka/bin/../libs/lz4-java-1.6.0.jar:/opt/kafka/bin/../libs/maven-artifact-3.6.1.jar:/opt/kafka/bin/../libs/metrics-core-2.2.0.jar:/opt/kafka/bin/../libs/netty-buffer-4.1.45.Final.jar:/opt/kafka/bin/../libs/netty-codec-4.1.45.Final.jar:/opt/kafka/bin/../libs/netty-common-4.1.45.Final.jar:/opt/kafka/bin/../libs/netty-handler-4.1.45.Final.jar:/opt/kafka/bin/../libs/netty-resolver-4.1.45.Final.jar:/opt/kafka/bin/../libs/netty-transport-4.1.45.Final.jar:/opt/kafka/bin/../libs/netty-transport-native-epoll-4.1.45.Final.jar:/opt/kafka/bin/../libs/netty-transport-native-unix-common-4.1.45.Final.jar:/opt/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/opt/kafka/bin/../libs/paranamer-2.8.jar:/opt/kafka/bin/../libs/plexus-utils-3.2.0.jar:/opt/kafka/bin/../libs/reflections-0.9.11.jar:/opt/kafka/bin/../libs/rocksdbjni-5.18.3.jar:/opt/kafka/bin/../libs/scala-collection-compat_2.12-2.1.2.jar:/opt/kafka/bin/../libs/scala-java8-compat_2.12-0.9.0.jar:/opt/kafka/bin/../libs/scala-library-2.12.10.jar:/opt/kafka/bin/../libs/scala-logging_2.12-3.9.2.jar:/opt/kafka/bin/../libs/scala-reflect-2.12.10.jar:/opt/kafka/bin/../libs/slf4j-api-1.7.28.jar:/opt/kafka/bin/../libs/slf4j-log4j12-1.7.28.jar:/opt/kafka/bin/../libs/snappy-java-1.1.7.3.jar:/opt/kafka/bin/../libs/validation-api-2.0.1.Final.jar:/opt/kafka/bin/../libs/zookeeper-3.5.7.jar:/opt/kafka/bin/../libs/zookeeper-jute-3.5.7.jar:/opt/kafka/bin/../libs/zstd-jni-1.4.3-1.jar (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,016 [myid:] - INFO  [main:Environment@100] - Server environment:java.vendor=Oracle Corporation
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:java.library.path=/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,016 [myid:] - INFO  [main:Environment@100] - Server environment:java.home=/usr/lib/jvm/java-7-openjdk-amd64/jre
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,016 [myid:] - INFO  [main:Environment@100] - Server environment:java.class.path=/opt/zookeeper-3.4.13/bin/../build/classes:/opt/zookeeper-3.4.13/bin/../build/lib/*.jar:/opt/zookeeper-3.4.13/bin/../lib/slf4j-log4j12-1.7.25.jar:/opt/zookeeper-3.4.13/bin/../lib/slf4j-api-1.7.25.jar:/opt/zookeeper-3.4.13/bin/../lib/netty-3.10.6.Final.jar:/opt/zookeeper-3.4.13/bin/../lib/log4j-1.2.17.jar:/opt/zookeeper-3.4.13/bin/../lib/jline-0.9.94.jar:/opt/zookeeper-3.4.13/bin/../lib/audience-annotations-0.5.0.jar:/opt/zookeeper-3.4.13/bin/../zookeeper-3.4.13.jar:/opt/zookeeper-3.4.13/bin/../src/java/lib/*.jar:/opt/zookeeper-3.4.13/bin/../conf:
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,016 [myid:] - INFO  [main:Environment@100] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,016 [myid:] - INFO  [main:Environment@100] - Server environment:java.io.tmpdir=/tmp
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,018 [myid:] - INFO  [main:Environment@100] - Server environment:java.compiler=<NA>
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,018 [myid:] - INFO  [main:Environment@100] - Server environment:os.name=Linux
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:os.version=5.5.16-1-MANJARO (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,018 [myid:] - INFO  [main:Environment@100] - Server environment:os.arch=amd64
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,019 [myid:] - INFO  [main:Environment@100] - Server environment:os.version=5.5.16-1-MANJARO
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,019 [myid:] - INFO  [main:Environment@100] - Server environment:user.name=root
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,019 [myid:] - INFO  [main:Environment@100] - Server environment:user.home=/root
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,019 [myid:] - INFO  [main:Environment@100] - Server environment:user.dir=/opt/zookeeper-3.4.13
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:user.dir=/ (org.apache.zookeeper.ZooKeeper)
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:os.memory.free=979MB (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,021 [myid:] - INFO  [main:ZooKeeperServer@836] - tickTime set to 2000
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,021 [myid:] - INFO  [main:ZooKeeperServer@845] - minSessionTimeout set to -1
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,021 [myid:] - INFO  [main:ZooKeeperServer@854] - maxSessionTimeout set to -1
[36mkafka_1          |[0m [2020-04-13 18:13:57,215] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,029 [myid:] - INFO  [main:ServerCnxnFactory@117] - Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory
[36mkafka_1          |[0m [2020-04-13 18:13:57,217] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3d51f06e (org.apache.zookeeper.ZooKeeper)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:56,033 [myid:] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2181
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:57,249 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@215] - Accepted socket connection from /192.168.64.4:43964
[36mkafka_1          |[0m [2020-04-13 18:13:57,221] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:57,260 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@949] - Client attempting to establish new session at /192.168.64.4:43964
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:57,263 [myid:] - INFO  [SyncThread:0:FileTxnLog@213] - Creating new log file: log.1
[36mkafka_1          |[0m [2020-04-13 18:13:57,225] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:57,281 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@694] - Established session 0x100004dfc100000 with negotiated timeout 6000 for client /192.168.64.4:43964
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:57,361 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x100004dfc100000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
[36mkafka_1          |[0m [2020-04-13 18:13:57,229] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:57,385 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x100004dfc100000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:57,398 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x100004dfc100000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
[36mkafka_1          |[0m [2020-04-13 18:13:57,231] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:57,619 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x100004dfc100000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
[36mkafka_1          |[0m [2020-04-13 18:13:57,241] INFO Opening socket connection to server zookeeper/192.168.64.2:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[36mkafka_1          |[0m [2020-04-13 18:13:57,246] INFO Socket connection established, initiating session, client: /192.168.64.4:43964, server: zookeeper/192.168.64.2:2181 (org.apache.zookeeper.ClientCnxn)
[36mkafka_1          |[0m [2020-04-13 18:13:57,283] INFO Session establishment complete on server zookeeper/192.168.64.2:2181, sessionid = 0x100004dfc100000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[36mkafka_1          |[0m [2020-04-13 18:13:57,289] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[36mkafka_1          |[0m [2020-04-13 18:13:57,629] INFO Cluster ID = okF80LrDSN-z5mN0v_DfCw (kafka.server.KafkaServer)
[36mkafka_1          |[0m [2020-04-13 18:13:57,632] WARN No meta.properties file under dir /kafka/kafka-logs-8d177c875ca7/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[36mkafka_1          |[0m [2020-04-13 18:13:57,695] INFO KafkaConfig values: 
[36mkafka_1          |[0m 	advertised.host.name = kafka
[36mkafka_1          |[0m 	advertised.listeners = null
[36mkafka_1          |[0m 	advertised.port = null
[36mkafka_1          |[0m 	alter.config.policy.class.name = null
[36mkafka_1          |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mkafka_1          |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mkafka_1          |[0m 	authorizer.class.name = 
[36mkafka_1          |[0m 	auto.create.topics.enable = true
[36mkafka_1          |[0m 	auto.leader.rebalance.enable = true
[36mkafka_1          |[0m 	background.threads = 10
[36mkafka_1          |[0m 	broker.id = -1
[36mkafka_1          |[0m 	broker.id.generation.enable = true
[36mkafka_1          |[0m 	broker.rack = null
[36mkafka_1          |[0m 	client.quota.callback.class = null
[36mkafka_1          |[0m 	compression.type = producer
[36mkafka_1          |[0m 	connection.failed.authentication.delay.ms = 100
[36mkafka_1          |[0m 	connections.max.idle.ms = 600000
[36mkafka_1          |[0m 	connections.max.reauth.ms = 0
[36mkafka_1          |[0m 	control.plane.listener.name = null
[36mkafka_1          |[0m 	controlled.shutdown.enable = true
[36mkafka_1          |[0m 	controlled.shutdown.max.retries = 3
[36mkafka_1          |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mkafka_1          |[0m 	controller.socket.timeout.ms = 30000
[36mkafka_1          |[0m 	create.topic.policy.class.name = null
[36mkafka_1          |[0m 	default.replication.factor = 1
[36mkafka_1          |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mkafka_1          |[0m 	delegation.token.expiry.time.ms = 86400000
[36mkafka_1          |[0m 	delegation.token.master.key = null
[36mkafka_1          |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mkafka_1          |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mkafka_1          |[0m 	delete.topic.enable = true
[36mkafka_1          |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mkafka_1          |[0m 	group.initial.rebalance.delay.ms = 0
[36mkafka_1          |[0m 	group.max.session.timeout.ms = 1800000
[36mkafka_1          |[0m 	group.max.size = 2147483647
[36mkafka_1          |[0m 	group.min.session.timeout.ms = 6000
[36mkafka_1          |[0m 	host.name = 
[36mkafka_1          |[0m 	inter.broker.listener.name = null
[36mkafka_1          |[0m 	inter.broker.protocol.version = 2.4-IV1
[36mkafka_1          |[0m 	kafka.metrics.polling.interval.secs = 10
[36mkafka_1          |[0m 	kafka.metrics.reporters = []
[36mkafka_1          |[0m 	leader.imbalance.check.interval.seconds = 300
[36mkafka_1          |[0m 	leader.imbalance.per.broker.percentage = 10
[36mkafka_1          |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
[36mkafka_1          |[0m 	listeners = null
[36mkafka_1          |[0m 	log.cleaner.backoff.ms = 15000
[36mkafka_1          |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mkafka_1          |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mkafka_1          |[0m 	log.cleaner.enable = true
[36mkafka_1          |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mkafka_1          |[0m 	log.cleaner.io.buffer.size = 524288
[36mkafka_1          |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mkafka_1          |[0m 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
[36mkafka_1          |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mkafka_1          |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mkafka_1          |[0m 	log.cleaner.threads = 1
[36mkafka_1          |[0m 	log.cleanup.policy = [delete]
[36mkafka_1          |[0m 	log.dir = /tmp/kafka-logs
[36mkafka_1          |[0m 	log.dirs = /kafka/kafka-logs-8d177c875ca7
[36mkafka_1          |[0m 	log.flush.interval.messages = 9223372036854775807
[36mkafka_1          |[0m 	log.flush.interval.ms = null
[36mkafka_1          |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mkafka_1          |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mkafka_1          |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mkafka_1          |[0m 	log.index.interval.bytes = 4096
[36mkafka_1          |[0m 	log.index.size.max.bytes = 10485760
[36mkafka_1          |[0m 	log.message.downconversion.enable = true
[36mkafka_1          |[0m 	log.message.format.version = 2.4-IV1
[36mkafka_1          |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mkafka_1          |[0m 	log.message.timestamp.type = CreateTime
[36mkafka_1          |[0m 	log.preallocate = false
[36mkafka_1          |[0m 	log.retention.bytes = -1
[36mkafka_1          |[0m 	log.retention.check.interval.ms = 300000
[36mkafka_1          |[0m 	log.retention.hours = 168
[36mkafka_1          |[0m 	log.retention.minutes = null
[36mkafka_1          |[0m 	log.retention.ms = null
[36mkafka_1          |[0m 	log.roll.hours = 168
[36mkafka_1          |[0m 	log.roll.jitter.hours = 0
[36mkafka_1          |[0m 	log.roll.jitter.ms = null
[36mkafka_1          |[0m 	log.roll.ms = null
[36mkafka_1          |[0m 	log.segment.bytes = 1073741824
[36mkafka_1          |[0m 	log.segment.delete.delay.ms = 60000
[36mkafka_1          |[0m 	max.connections = 2147483647
[36mkafka_1          |[0m 	max.connections.per.ip = 2147483647
[36mkafka_1          |[0m 	max.connections.per.ip.overrides = 
[36mkafka_1          |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mkafka_1          |[0m 	message.max.bytes = 1000012
[36mkafka_1          |[0m 	metric.reporters = []
[36mkafka_1          |[0m 	metrics.num.samples = 2
[36mkafka_1          |[0m 	metrics.recording.level = INFO
[36mkafka_1          |[0m 	metrics.sample.window.ms = 30000
[36mkafka_1          |[0m 	min.insync.replicas = 1
[36mkafka_1          |[0m 	num.io.threads = 8
[36mkafka_1          |[0m 	num.network.threads = 3
[36mkafka_1          |[0m 	num.partitions = 1
[36mkafka_1          |[0m 	num.recovery.threads.per.data.dir = 1
[36mkafka_1          |[0m 	num.replica.alter.log.dirs.threads = null
[36mkafka_1          |[0m 	num.replica.fetchers = 1
[36mkafka_1          |[0m 	offset.metadata.max.bytes = 4096
[36mkafka_1          |[0m 	offsets.commit.required.acks = -1
[36mkafka_1          |[0m 	offsets.commit.timeout.ms = 5000
[36mkafka_1          |[0m 	offsets.load.buffer.size = 5242880
[36mkafka_1          |[0m 	offsets.retention.check.interval.ms = 600000
[36mkafka_1          |[0m 	offsets.retention.minutes = 10080
[36mkafka_1          |[0m 	offsets.topic.compression.codec = 0
[36mkafka_1          |[0m 	offsets.topic.num.partitions = 50
[36mkafka_1          |[0m 	offsets.topic.replication.factor = 1
[36mkafka_1          |[0m 	offsets.topic.segment.bytes = 104857600
[36mkafka_1          |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mkafka_1          |[0m 	password.encoder.iterations = 4096
[36mkafka_1          |[0m 	password.encoder.key.length = 128
[36mkafka_1          |[0m 	password.encoder.keyfactory.algorithm = null
[36mkafka_1          |[0m 	password.encoder.old.secret = null
[36mkafka_1          |[0m 	password.encoder.secret = null
[36mkafka_1          |[0m 	port = 9092
[36mkafka_1          |[0m 	principal.builder.class = null
[36mkafka_1          |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mkafka_1          |[0m 	queued.max.request.bytes = -1
[36mkafka_1          |[0m 	queued.max.requests = 500
[36mkafka_1          |[0m 	quota.consumer.default = 9223372036854775807
[36mkafka_1          |[0m 	quota.producer.default = 9223372036854775807
[36mkafka_1          |[0m 	quota.window.num = 11
[36mkafka_1          |[0m 	quota.window.size.seconds = 1
[36mkafka_1          |[0m 	replica.fetch.backoff.ms = 1000
[36mkafka_1          |[0m 	replica.fetch.max.bytes = 1048576
[36mkafka_1          |[0m 	replica.fetch.min.bytes = 1
[36mkafka_1          |[0m 	replica.fetch.response.max.bytes = 10485760
[36mkafka_1          |[0m 	replica.fetch.wait.max.ms = 500
[36mkafka_1          |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mkafka_1          |[0m 	replica.lag.time.max.ms = 10000
[36mkafka_1          |[0m 	replica.selector.class = null
[36mkafka_1          |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mkafka_1          |[0m 	replica.socket.timeout.ms = 30000
[36mkafka_1          |[0m 	replication.quota.window.num = 11
[36mkafka_1          |[0m 	replication.quota.window.size.seconds = 1
[36mkafka_1          |[0m 	request.timeout.ms = 30000
[36mkafka_1          |[0m 	reserved.broker.max.id = 1000
[36mkafka_1          |[0m 	sasl.client.callback.handler.class = null
[36mkafka_1          |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mkafka_1          |[0m 	sasl.jaas.config = null
[36mkafka_1          |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka_1          |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka_1          |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mkafka_1          |[0m 	sasl.kerberos.service.name = null
[36mkafka_1          |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka_1          |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka_1          |[0m 	sasl.login.callback.handler.class = null
[36mkafka_1          |[0m 	sasl.login.class = null
[36mkafka_1          |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka_1          |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka_1          |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka_1          |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka_1          |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mkafka_1          |[0m 	sasl.server.callback.handler.class = null
[36mkafka_1          |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mkafka_1          |[0m 	security.providers = null
[36mkafka_1          |[0m 	socket.receive.buffer.bytes = 102400
[36mkafka_1          |[0m 	socket.request.max.bytes = 104857600
[36mkafka_1          |[0m 	socket.send.buffer.bytes = 102400
[36mkafka_1          |[0m 	ssl.cipher.suites = []
[36mkafka_1          |[0m 	ssl.client.auth = none
[36mkafka_1          |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36mkafka_1          |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka_1          |[0m 	ssl.key.password = null
[36mkafka_1          |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka_1          |[0m 	ssl.keystore.location = null
[36mkafka_1          |[0m 	ssl.keystore.password = null
[36mkafka_1          |[0m 	ssl.keystore.type = JKS
[36mkafka_1          |[0m 	ssl.principal.mapping.rules = DEFAULT
[36mkafka_1          |[0m 	ssl.protocol = TLS
[36mkafka_1          |[0m 	ssl.provider = null
[36mkafka_1          |[0m 	ssl.secure.random.implementation = null
[36mkafka_1          |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka_1          |[0m 	ssl.truststore.location = null
[36mkafka_1          |[0m 	ssl.truststore.password = null
[36mkafka_1          |[0m 	ssl.truststore.type = JKS
[36mkafka_1          |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[36mkafka_1          |[0m 	transaction.max.timeout.ms = 900000
[36mkafka_1          |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mkafka_1          |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mkafka_1          |[0m 	transaction.state.log.min.isr = 1
[36mkafka_1          |[0m 	transaction.state.log.num.partitions = 50
[36mkafka_1          |[0m 	transaction.state.log.replication.factor = 1
[36mkafka_1          |[0m 	transaction.state.log.segment.bytes = 104857600
[36mkafka_1          |[0m 	transactional.id.expiration.ms = 604800000
[36mkafka_1          |[0m 	unclean.leader.election.enable = false
[36mkafka_1          |[0m 	zookeeper.connect = zookeeper:2181
[36mkafka_1          |[0m 	zookeeper.connection.timeout.ms = 6000
[36mkafka_1          |[0m 	zookeeper.max.in.flight.requests = 10
[36mkafka_1          |[0m 	zookeeper.session.timeout.ms = 6000
[36mkafka_1          |[0m 	zookeeper.set.acl = false
[36mkafka_1          |[0m 	zookeeper.sync.time.ms = 2000
[36mkafka_1          |[0m  (kafka.server.KafkaConfig)
[36mkafka_1          |[0m [2020-04-13 18:13:57,706] INFO KafkaConfig values: 
[36mkafka_1          |[0m 	advertised.host.name = kafka
[36mkafka_1          |[0m 	advertised.listeners = null
[36mkafka_1          |[0m 	advertised.port = null
[36mkafka_1          |[0m 	alter.config.policy.class.name = null
[36mkafka_1          |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mkafka_1          |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mkafka_1          |[0m 	authorizer.class.name = 
[36mkafka_1          |[0m 	auto.create.topics.enable = true
[36mkafka_1          |[0m 	auto.leader.rebalance.enable = true
[36mkafka_1          |[0m 	background.threads = 10
[36mkafka_1          |[0m 	broker.id = -1
[36mkafka_1          |[0m 	broker.id.generation.enable = true
[36mkafka_1          |[0m 	broker.rack = null
[36mkafka_1          |[0m 	client.quota.callback.class = null
[36mkafka_1          |[0m 	compression.type = producer
[36mkafka_1          |[0m 	connection.failed.authentication.delay.ms = 100
[36mkafka_1          |[0m 	connections.max.idle.ms = 600000
[36mkafka_1          |[0m 	connections.max.reauth.ms = 0
[36mkafka_1          |[0m 	control.plane.listener.name = null
[36mkafka_1          |[0m 	controlled.shutdown.enable = true
[36mkafka_1          |[0m 	controlled.shutdown.max.retries = 3
[36mkafka_1          |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mkafka_1          |[0m 	controller.socket.timeout.ms = 30000
[36mkafka_1          |[0m 	create.topic.policy.class.name = null
[36mkafka_1          |[0m 	default.replication.factor = 1
[36mkafka_1          |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mkafka_1          |[0m 	delegation.token.expiry.time.ms = 86400000
[36mkafka_1          |[0m 	delegation.token.master.key = null
[36mkafka_1          |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mkafka_1          |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mkafka_1          |[0m 	delete.topic.enable = true
[36mkafka_1          |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mkafka_1          |[0m 	group.initial.rebalance.delay.ms = 0
[36mkafka_1          |[0m 	group.max.session.timeout.ms = 1800000
[36mkafka_1          |[0m 	group.max.size = 2147483647
[36mkafka_1          |[0m 	group.min.session.timeout.ms = 6000
[36mkafka_1          |[0m 	host.name = 
[36mkafka_1          |[0m 	inter.broker.listener.name = null
[36mkafka_1          |[0m 	inter.broker.protocol.version = 2.4-IV1
[36mkafka_1          |[0m 	kafka.metrics.polling.interval.secs = 10
[36mkafka_1          |[0m 	kafka.metrics.reporters = []
[36mkafka_1          |[0m 	leader.imbalance.check.interval.seconds = 300
[36mkafka_1          |[0m 	leader.imbalance.per.broker.percentage = 10
[36mkafka_1          |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
[36mkafka_1          |[0m 	listeners = null
[36mkafka_1          |[0m 	log.cleaner.backoff.ms = 15000
[36mkafka_1          |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mkafka_1          |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mkafka_1          |[0m 	log.cleaner.enable = true
[36mkafka_1          |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mkafka_1          |[0m 	log.cleaner.io.buffer.size = 524288
[36mkafka_1          |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mkafka_1          |[0m 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
[36mkafka_1          |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mkafka_1          |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mkafka_1          |[0m 	log.cleaner.threads = 1
[36mkafka_1          |[0m 	log.cleanup.policy = [delete]
[36mkafka_1          |[0m 	log.dir = /tmp/kafka-logs
[36mkafka_1          |[0m 	log.dirs = /kafka/kafka-logs-8d177c875ca7
[36mkafka_1          |[0m 	log.flush.interval.messages = 9223372036854775807
[36mkafka_1          |[0m 	log.flush.interval.ms = null
[36mkafka_1          |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mkafka_1          |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mkafka_1          |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mkafka_1          |[0m 	log.index.interval.bytes = 4096
[36mkafka_1          |[0m 	log.index.size.max.bytes = 10485760
[36mkafka_1          |[0m 	log.message.downconversion.enable = true
[36mkafka_1          |[0m 	log.message.format.version = 2.4-IV1
[36mkafka_1          |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mkafka_1          |[0m 	log.message.timestamp.type = CreateTime
[36mkafka_1          |[0m 	log.preallocate = false
[36mkafka_1          |[0m 	log.retention.bytes = -1
[36mkafka_1          |[0m 	log.retention.check.interval.ms = 300000
[36mkafka_1          |[0m 	log.retention.hours = 168
[36mkafka_1          |[0m 	log.retention.minutes = null
[36mkafka_1          |[0m 	log.retention.ms = null
[36mkafka_1          |[0m 	log.roll.hours = 168
[36mkafka_1          |[0m 	log.roll.jitter.hours = 0
[36mkafka_1          |[0m 	log.roll.jitter.ms = null
[36mkafka_1          |[0m 	log.roll.ms = null
[36mkafka_1          |[0m 	log.segment.bytes = 1073741824
[36mkafka_1          |[0m 	log.segment.delete.delay.ms = 60000
[36mkafka_1          |[0m 	max.connections = 2147483647
[36mkafka_1          |[0m 	max.connections.per.ip = 2147483647
[36mkafka_1          |[0m 	max.connections.per.ip.overrides = 
[36mkafka_1          |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mkafka_1          |[0m 	message.max.bytes = 1000012
[36mkafka_1          |[0m 	metric.reporters = []
[36mkafka_1          |[0m 	metrics.num.samples = 2
[36mkafka_1          |[0m 	metrics.recording.level = INFO
[36mkafka_1          |[0m 	metrics.sample.window.ms = 30000
[36mkafka_1          |[0m 	min.insync.replicas = 1
[36mkafka_1          |[0m 	num.io.threads = 8
[36mkafka_1          |[0m 	num.network.threads = 3
[36mkafka_1          |[0m 	num.partitions = 1
[36mkafka_1          |[0m 	num.recovery.threads.per.data.dir = 1
[36mkafka_1          |[0m 	num.replica.alter.log.dirs.threads = null
[36mkafka_1          |[0m 	num.replica.fetchers = 1
[36mkafka_1          |[0m 	offset.metadata.max.bytes = 4096
[36mkafka_1          |[0m 	offsets.commit.required.acks = -1
[36mkafka_1          |[0m 	offsets.commit.timeout.ms = 5000
[36mkafka_1          |[0m 	offsets.load.buffer.size = 5242880
[36mkafka_1          |[0m 	offsets.retention.check.interval.ms = 600000
[36mkafka_1          |[0m 	offsets.retention.minutes = 10080
[36mkafka_1          |[0m 	offsets.topic.compression.codec = 0
[36mkafka_1          |[0m 	offsets.topic.num.partitions = 50
[36mkafka_1          |[0m 	offsets.topic.replication.factor = 1
[36mkafka_1          |[0m 	offsets.topic.segment.bytes = 104857600
[36mkafka_1          |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mkafka_1          |[0m 	password.encoder.iterations = 4096
[36mkafka_1          |[0m 	password.encoder.key.length = 128
[36mkafka_1          |[0m 	password.encoder.keyfactory.algorithm = null
[36mkafka_1          |[0m 	password.encoder.old.secret = null
[36mkafka_1          |[0m 	password.encoder.secret = null
[36mkafka_1          |[0m 	port = 9092
[36mkafka_1          |[0m 	principal.builder.class = null
[36mkafka_1          |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mkafka_1          |[0m 	queued.max.request.bytes = -1
[36mkafka_1          |[0m 	queued.max.requests = 500
[36mkafka_1          |[0m 	quota.consumer.default = 9223372036854775807
[36mkafka_1          |[0m 	quota.producer.default = 9223372036854775807
[36mkafka_1          |[0m 	quota.window.num = 11
[36mkafka_1          |[0m 	quota.window.size.seconds = 1
[36mkafka_1          |[0m 	replica.fetch.backoff.ms = 1000
[36mkafka_1          |[0m 	replica.fetch.max.bytes = 1048576
[36mkafka_1          |[0m 	replica.fetch.min.bytes = 1
[36mkafka_1          |[0m 	replica.fetch.response.max.bytes = 10485760
[36mkafka_1          |[0m 	replica.fetch.wait.max.ms = 500
[36mkafka_1          |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mkafka_1          |[0m 	replica.lag.time.max.ms = 10000
[36mkafka_1          |[0m 	replica.selector.class = null
[36mkafka_1          |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mkafka_1          |[0m 	replica.socket.timeout.ms = 30000
[36mkafka_1          |[0m 	replication.quota.window.num = 11
[36mkafka_1          |[0m 	replication.quota.window.size.seconds = 1
[36mkafka_1          |[0m 	request.timeout.ms = 30000
[36mkafka_1          |[0m 	reserved.broker.max.id = 1000
[36mkafka_1          |[0m 	sasl.client.callback.handler.class = null
[36mkafka_1          |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mkafka_1          |[0m 	sasl.jaas.config = null
[36mkafka_1          |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka_1          |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka_1          |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mkafka_1          |[0m 	sasl.kerberos.service.name = null
[36mkafka_1          |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka_1          |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka_1          |[0m 	sasl.login.callback.handler.class = null
[36mkafka_1          |[0m 	sasl.login.class = null
[36mkafka_1          |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka_1          |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka_1          |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka_1          |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka_1          |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mkafka_1          |[0m 	sasl.server.callback.handler.class = null
[36mkafka_1          |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mkafka_1          |[0m 	security.providers = null
[36mkafka_1          |[0m 	socket.receive.buffer.bytes = 102400
[36mkafka_1          |[0m 	socket.request.max.bytes = 104857600
[36mkafka_1          |[0m 	socket.send.buffer.bytes = 102400
[36mkafka_1          |[0m 	ssl.cipher.suites = []
[36mkafka_1          |[0m 	ssl.client.auth = none
[36mkafka_1          |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[36mkafka_1          |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka_1          |[0m 	ssl.key.password = null
[36mkafka_1          |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka_1          |[0m 	ssl.keystore.location = null
[36mkafka_1          |[0m 	ssl.keystore.password = null
[36mkafka_1          |[0m 	ssl.keystore.type = JKS
[36mkafka_1          |[0m 	ssl.principal.mapping.rules = DEFAULT
[36mkafka_1          |[0m 	ssl.protocol = TLS
[36mkafka_1          |[0m 	ssl.provider = null
[36mkafka_1          |[0m 	ssl.secure.random.implementation = null
[36mkafka_1          |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka_1          |[0m 	ssl.truststore.location = null
[36mkafka_1          |[0m 	ssl.truststore.password = null
[36mkafka_1          |[0m 	ssl.truststore.type = JKS
[36mkafka_1          |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[36mkafka_1          |[0m 	transaction.max.timeout.ms = 900000
[36mkafka_1          |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mkafka_1          |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mkafka_1          |[0m 	transaction.state.log.min.isr = 1
[36mkafka_1          |[0m 	transaction.state.log.num.partitions = 50
[36mkafka_1          |[0m 	transaction.state.log.replication.factor = 1
[36mkafka_1          |[0m 	transaction.state.log.segment.bytes = 104857600
[36mkafka_1          |[0m 	transactional.id.expiration.ms = 604800000
[36mkafka_1          |[0m 	unclean.leader.election.enable = false
[36mkafka_1          |[0m 	zookeeper.connect = zookeeper:2181
[36mkafka_1          |[0m 	zookeeper.connection.timeout.ms = 6000
[36mkafka_1          |[0m 	zookeeper.max.in.flight.requests = 10
[36mkafka_1          |[0m 	zookeeper.session.timeout.ms = 6000
[36mkafka_1          |[0m 	zookeeper.set.acl = false
[36mkafka_1          |[0m 	zookeeper.sync.time.ms = 2000
[36mkafka_1          |[0m  (kafka.server.KafkaConfig)
[36mkafka_1          |[0m [2020-04-13 18:13:57,729] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:57,729] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:57,730] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:57,750] INFO Log directory /kafka/kafka-logs-8d177c875ca7 not found, creating it. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:57,756] INFO Loading logs. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:57,763] INFO Logs loading complete in 7 ms. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:57,779] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:57,786] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:58,170] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[36mkafka_1          |[0m [2020-04-13 18:13:58,199] INFO [SocketServer brokerId=1001] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[36mkafka_1          |[0m [2020-04-13 18:13:58,201] INFO [SocketServer brokerId=1001] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[36mkafka_1          |[0m [2020-04-13 18:13:58,219] INFO [ExpirationReaper-1001-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:58,221] INFO [ExpirationReaper-1001-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:58,222] INFO [ExpirationReaper-1001-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:58,222] INFO [ExpirationReaper-1001-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:58,234] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[36mkafka_1          |[0m [2020-04-13 18:13:58,255] INFO Creating /brokers/ids/1001 (is it secure? false) (kafka.zk.KafkaZkClient)
[36mkafka_1          |[0m [2020-04-13 18:13:58,279] INFO Stat of the created znode at /brokers/ids/1001 is: 25,25,1586801638266,1586801638266,1,0,0,72057928979316736,180,0,25
[36mkafka_1          |[0m  (kafka.zk.KafkaZkClient)
[36mkafka_1          |[0m [2020-04-13 18:13:58,280] INFO Registered broker 1001 at path /brokers/ids/1001 with addresses: ArrayBuffer(EndPoint(kafka,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)
[36mkafka_1          |[0m [2020-04-13 18:13:58,342] INFO [ExpirationReaper-1001-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:58,344] INFO [ExpirationReaper-1001-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:58,345] INFO [ExpirationReaper-1001-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:58,362] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[36mkafka_1          |[0m [2020-04-13 18:13:58,380] INFO [GroupCoordinator 1001]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[36mkafka_1          |[0m [2020-04-13 18:13:58,382] INFO [GroupCoordinator 1001]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[36mkafka_1          |[0m [2020-04-13 18:13:58,388] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 7 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:13:58,396] INFO [ProducerId Manager 1001]: Acquired new producerId block (brokerId:1001,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[36mkafka_1          |[0m [2020-04-13 18:13:58,429] INFO [TransactionCoordinator id=1001] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[36mkafka_1          |[0m [2020-04-13 18:13:58,431] INFO [Transaction Marker Channel Manager 1001]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[36mkafka_1          |[0m [2020-04-13 18:13:58,432] INFO [TransactionCoordinator id=1001] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[36mkafka_1          |[0m [2020-04-13 18:13:58,468] INFO [ExpirationReaper-1001-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka_1          |[0m [2020-04-13 18:13:58,486] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:58,489 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@596] - Got user-level KeeperException when processing sessionid:0x100004dfc100000 type:multi cxid:0x32 zxid:0x1d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
[36mkafka_1          |[0m [2020-04-13 18:13:58,502] INFO [SocketServer brokerId=1001] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[36mkafka_1          |[0m [2020-04-13 18:13:58,509] INFO Kafka version: 2.4.1 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka_1          |[0m [2020-04-13 18:13:58,509] INFO Kafka commitId: c57222ae8cd7866b (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka_1          |[0m [2020-04-13 18:13:58,509] INFO Kafka startTimeMs: 1586801638503 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka_1          |[0m [2020-04-13 18:13:58,510] INFO [KafkaServer id=1001] started (kafka.server.KafkaServer)
[35mnode2_1          |[0m %7|1586801638.540|CONNECT|rdkafka#consumer-1| [thrd:main]: kafka:9092/bootstrap: Selected for cluster connection: coordinator query (broker has 2 connection attempt(s))
[35mnode2_1          |[0m %7|1586801638.540|CGRPQUERY|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": no broker available for coordinator query: intervaled in state query-coord
[35mnode2_1          |[0m %7|1586801638.540|CONNECT|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Received CONNECT op
[35mnode2_1          |[0m %7|1586801638.540|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT
[35mnode2_1          |[0m %7|1586801638.540|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801638.540|CONNECT|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: broker in state TRY_CONNECT connecting
[35mnode2_1          |[0m %7|1586801638.540|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT
[35mnode2_1          |[0m %7|1586801638.540|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801638.541|CONNECT|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connecting to ipv4#192.168.64.4:9092 (plaintext) with socket 10
[35mnode2_1          |[0m %7|1586801638.541|CONNECT|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connected to ipv4#192.168.64.4:9092
[35mnode2_1          |[0m %7|1586801638.541|CONNECTED|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Connected (#1)
[35mnode2_1          |[0m %7|1586801638.541|FEATURE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion
[35mnode2_1          |[0m %7|1586801638.541|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY
[35mnode2_1          |[0m %7|1586801638.541|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801638.541|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Sent ApiVersionRequest (v3, 62 bytes @ 0, CorrId 1)
[35mnode2_1          |[0m %7|1586801638.541|CONNECT|rdkafka#consumer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection
[35mnode2_1          |[0m %7|1586801638.572|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Received ApiVersionResponse (v3, 344 bytes, CorrId 1, rtt 31.08ms)
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker API support:
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey Produce (0) Versions 0..8
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey Fetch (1) Versions 0..11
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey Offset (2) Versions 0..5
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey Metadata (3) Versions 0..9
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey LeaderAndIsr (4) Versions 0..4
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey StopReplica (5) Versions 0..2
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey UpdateMetadata (6) Versions 0..6
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey ControlledShutdown (7) Versions 0..3
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey OffsetCommit (8) Versions 0..8
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey OffsetFetch (9) Versions 0..6
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey FindCoordinator (10) Versions 0..3
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey JoinGroup (11) Versions 0..6
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey Heartbeat (12) Versions 0..4
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey LeaveGroup (13) Versions 0..4
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey SyncGroup (14) Versions 0..4
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey DescribeGroups (15) Versions 0..5
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey ListGroups (16) Versions 0..3
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey SaslHandshake (17) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey ApiVersion (18) Versions 0..3
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey CreateTopics (19) Versions 0..5
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey DeleteTopics (20) Versions 0..4
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey DeleteRecords (21) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey InitProducerId (22) Versions 0..2
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey OffsetForLeaderEpoch (23) Versions 0..3
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey AddPartitionsToTxn (24) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey AddOffsetsToTxn (25) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey EndTxn (26) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey WriteTxnMarkers (27) Versions 0..0
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey TxnOffsetCommit (28) Versions 0..2
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey DescribeAcls (29) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey CreateAcls (30) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey DeleteAcls (31) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey DescribeConfigs (32) Versions 0..2
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey AlterConfigs (33) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey AlterReplicaLogDirs (34) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey DescribeLogDirs (35) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey SaslAuthenticate (36) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey CreatePartitions (37) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey CreateDelegationToken (38) Versions 0..2
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey RenewDelegationToken (39) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey ExpireDelegationToken (40) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey DescribeDelegationToken (41) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey DeleteGroups (42) Versions 0..2
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey Unknown-43? (43) Versions 0..2
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey Unknown-44? (44) Versions 0..1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey Unknown-45? (45) Versions 0..0
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey Unknown-46? (46) Versions 0..0
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:   ApiKey Unknown-47? (47) Versions 0..0
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature MsgVer1: Produce (2..2) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature MsgVer1: Fetch (2..2) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature MsgVer1
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature MsgVer2: Produce (3..3) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature MsgVer2: Fetch (4..4) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature MsgVer2
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature ApiVersion: ApiVersion (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature ApiVersion
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature BrokerGroupCoordinator: FindCoordinator (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature BrokerGroupCoordinator
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature BrokerBalancedConsumer: FindCoordinator (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature BrokerBalancedConsumer: OffsetCommit (1..2) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature BrokerBalancedConsumer: OffsetFetch (1..1) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature BrokerBalancedConsumer: JoinGroup (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature BrokerBalancedConsumer: SyncGroup (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature BrokerBalancedConsumer: Heartbeat (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature BrokerBalancedConsumer: LeaveGroup (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature BrokerBalancedConsumer
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature ThrottleTime: Produce (1..2) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature ThrottleTime: Fetch (1..2) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature ThrottleTime
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature Sasl: JoinGroup (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature Sasl
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature SaslHandshake: SaslHandshake (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature SaslHandshake
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature LZ4: FindCoordinator (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature LZ4
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature OffsetTime: Offset (1..1) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature OffsetTime
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature IdempotentProducer: InitProducerId (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature IdempotentProducer
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature ZSTD: Produce (7..7) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature ZSTD: Fetch (10..10) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature ZSTD
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature SaslAuthReq: SaslHandshake (1..1) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap:  Feature SaslAuthReq: SaslAuthenticate (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801638.572|APIVERSION|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Enabling feature SaslAuthReq
[35mnode2_1          |[0m %7|1586801638.572|FEATURE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Updated enabled protocol features to MsgVer1,ApiVersion,BrokerBalancedConsumer,ThrottleTime,Sasl,SaslHandshake,BrokerGroupCoordinator,LZ4,OffsetTime,MsgVer2,IdempotentProducer,ZSTD,SaslAuthReq
[35mnode2_1          |[0m %7|1586801638.572|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Broker changed state APIVERSION_QUERY -> UP
[35mnode2_1          |[0m %7|1586801638.572|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801638.572|METADATA|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Request metadata for brokers only: connected
[35mnode2_1          |[0m %7|1586801638.572|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Sent MetadataRequest (v2, 25 bytes @ 0, CorrId 2)
[35mnode2_1          |[0m %7|1586801638.599|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: Received MetadataResponse (v2, 53 bytes, CorrId 2, rtt 26.25ms)
[35mnode2_1          |[0m %7|1586801638.599|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/bootstrap: ===== Received metadata: connected =====
[35mnode2_1          |[0m %7|1586801638.599|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/bootstrap: ClusterId: okF80LrDSN-z5mN0v_DfCw, ControllerId: 1001
[35mnode2_1          |[0m %7|1586801638.599|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/bootstrap: 1 brokers, 0 topics
[35mnode2_1          |[0m %7|1586801638.599|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/bootstrap:   Broker #0/1: kafka:9092 NodeId 1001
[35mnode2_1          |[0m %7|1586801638.599|CLUSTERID|rdkafka#consumer-1| [thrd:main]: kafka:9092/bootstrap: ClusterId update "" -> "okF80LrDSN-z5mN0v_DfCw"
[35mnode2_1          |[0m %7|1586801638.599|CONTROLLERID|rdkafka#consumer-1| [thrd:main]: kafka:9092/bootstrap: ControllerId update -1 -> 1001
[35mnode2_1          |[0m %7|1586801638.599|BROADCAST|rdkafka#consumer-1| [thrd:main]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801638.599|UPDATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/bootstrap: NodeId changed from -1 to 1001
[35mnode2_1          |[0m %7|1586801638.599|UPDATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Name changed from kafka:9092/bootstrap to kafka:9092/1001
[35mnode2_1          |[0m %7|1586801638.599|LEADER|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Mapped 0 partition(s) to broker
[35mnode2_1          |[0m %7|1586801638.599|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Broker changed state UP -> UPDATE
[35mnode2_1          |[0m %7|1586801638.599|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801638.599|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801638.599|STATE|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Broker changed state UPDATE -> UP
[35mnode2_1          |[0m %7|1586801638.599|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[36mkafka_1          |[0m [2020-04-13 18:13:58,615] INFO Creating topic striot-queue with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:58,616 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x100004dfc100000 type:setData cxid:0x3f zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/striot-queue Error:KeeperErrorCode = NoNode for /config/topics/striot-queue
[36mkafka_1          |[0m [2020-04-13 18:13:58,637] INFO [KafkaApi-1001] Auto creation of topic striot-queue with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[36mkafka_1          |[0m [2020-04-13 18:13:58,707] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions Set(striot-queue-0) (kafka.server.ReplicaFetcherManager)
[36mkafka_1          |[0m [2020-04-13 18:13:58,787] INFO [Log partition=striot-queue-0, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:58,793] INFO [Log partition=striot-queue-0, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 43 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:58,794] INFO Created log for partition striot-queue-0 in /kafka/kafka-logs-8d177c875ca7/striot-queue-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:58,795] INFO [Partition striot-queue-0 broker=1001] No checkpointed highwatermark is found for partition striot-queue-0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:58,796] INFO [Partition striot-queue-0 broker=1001] Log loaded for partition striot-queue-0 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:58,796] INFO [Partition striot-queue-0 broker=1001] striot-queue-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[35mnode2_1          |[0m %7|1586801639.541|CGRPQUERY|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Group "striot_con_group": querying for coordinator: intervaled in state query-coord
[35mnode2_1          |[0m %7|1586801639.541|CGRPSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed state query-coord -> wait-coord (v1, join-state init)
[35mnode2_1          |[0m %7|1586801639.541|BROADCAST|rdkafka#consumer-1| [thrd:main]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801639.541|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FindCoordinatorRequest (v2, 40 bytes @ 0, CorrId 3)
[36mkafka_1          |[0m [2020-04-13 18:13:59,570] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(23 -> ArrayBuffer(1001), 32 -> ArrayBuffer(1001), 41 -> ArrayBuffer(1001), 17 -> ArrayBuffer(1001), 8 -> ArrayBuffer(1001), 35 -> ArrayBuffer(1001), 44 -> ArrayBuffer(1001), 26 -> ArrayBuffer(1001), 11 -> ArrayBuffer(1001), 29 -> ArrayBuffer(1001), 38 -> ArrayBuffer(1001), 47 -> ArrayBuffer(1001), 20 -> ArrayBuffer(1001), 2 -> ArrayBuffer(1001), 5 -> ArrayBuffer(1001), 14 -> ArrayBuffer(1001), 46 -> ArrayBuffer(1001), 49 -> ArrayBuffer(1001), 40 -> ArrayBuffer(1001), 13 -> ArrayBuffer(1001), 4 -> ArrayBuffer(1001), 22 -> ArrayBuffer(1001), 31 -> ArrayBuffer(1001), 16 -> ArrayBuffer(1001), 7 -> ArrayBuffer(1001), 43 -> ArrayBuffer(1001), 25 -> ArrayBuffer(1001), 34 -> ArrayBuffer(1001), 10 -> ArrayBuffer(1001), 37 -> ArrayBuffer(1001), 1 -> ArrayBuffer(1001), 19 -> ArrayBuffer(1001), 28 -> ArrayBuffer(1001), 45 -> ArrayBuffer(1001), 27 -> ArrayBuffer(1001), 36 -> ArrayBuffer(1001), 18 -> ArrayBuffer(1001), 9 -> ArrayBuffer(1001), 21 -> ArrayBuffer(1001), 48 -> ArrayBuffer(1001), 3 -> ArrayBuffer(1001), 12 -> ArrayBuffer(1001), 30 -> ArrayBuffer(1001), 39 -> ArrayBuffer(1001), 15 -> ArrayBuffer(1001), 42 -> ArrayBuffer(1001), 24 -> ArrayBuffer(1001), 6 -> ArrayBuffer(1001), 33 -> ArrayBuffer(1001), 0 -> ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[36;1mzookeeper_1      |[0m 2020-04-13 18:13:59,571 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@653] - Got user-level KeeperException when processing sessionid:0x100004dfc100000 type:setData cxid:0x4c zxid:0x24 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
[36mkafka_1          |[0m [2020-04-13 18:13:59,590] INFO [KafkaApi-1001] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[35mnode2_1          |[0m %7|1586801639.595|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FindCoordinatorResponse (v2, 51 bytes, CorrId 3, rtt 53.41ms)
[35mnode2_1          |[0m %7|1586801639.595|CGRPCOORD|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Group "striot_con_group" FindCoordinator response error: COORDINATOR_NOT_AVAILABLE: The coordinator is not available.
[35mnode2_1          |[0m %7|1586801639.595|CGRPSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed state wait-coord -> query-coord (v1, join-state init)
[35mnode2_1          |[0m %7|1586801639.595|BROADCAST|rdkafka#consumer-1| [thrd:main]: Broadcasting state change
[36mkafka_1          |[0m [2020-04-13 18:13:59,826] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,830] INFO [Log partition=__consumer_offsets-0, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,830] INFO [Log partition=__consumer_offsets-0, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,831] INFO Created log for partition __consumer_offsets-0 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,832] INFO [Partition __consumer_offsets-0 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,832] INFO [Partition __consumer_offsets-0 broker=1001] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,832] INFO [Partition __consumer_offsets-0 broker=1001] __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,846] INFO [Log partition=__consumer_offsets-29, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,847] INFO [Log partition=__consumer_offsets-29, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,848] INFO Created log for partition __consumer_offsets-29 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-29 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,848] INFO [Partition __consumer_offsets-29 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,848] INFO [Partition __consumer_offsets-29 broker=1001] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,848] INFO [Partition __consumer_offsets-29 broker=1001] __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,863] INFO [Log partition=__consumer_offsets-48, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,864] INFO [Log partition=__consumer_offsets-48, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,865] INFO Created log for partition __consumer_offsets-48 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-48 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,865] INFO [Partition __consumer_offsets-48 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,866] INFO [Partition __consumer_offsets-48 broker=1001] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,866] INFO [Partition __consumer_offsets-48 broker=1001] __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,875] INFO [Log partition=__consumer_offsets-10, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,876] INFO [Log partition=__consumer_offsets-10, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,876] INFO Created log for partition __consumer_offsets-10 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-10 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,877] INFO [Partition __consumer_offsets-10 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,877] INFO [Partition __consumer_offsets-10 broker=1001] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,877] INFO [Partition __consumer_offsets-10 broker=1001] __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,892] INFO [Log partition=__consumer_offsets-45, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,892] INFO [Log partition=__consumer_offsets-45, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,894] INFO Created log for partition __consumer_offsets-45 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-45 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,894] INFO [Partition __consumer_offsets-45 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,894] INFO [Partition __consumer_offsets-45 broker=1001] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,894] INFO [Partition __consumer_offsets-45 broker=1001] __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,909] INFO [Log partition=__consumer_offsets-26, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,909] INFO [Log partition=__consumer_offsets-26, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,910] INFO Created log for partition __consumer_offsets-26 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-26 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,910] INFO [Partition __consumer_offsets-26 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,911] INFO [Partition __consumer_offsets-26 broker=1001] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,911] INFO [Partition __consumer_offsets-26 broker=1001] __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,920] INFO [Log partition=__consumer_offsets-7, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,920] INFO [Log partition=__consumer_offsets-7, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,921] INFO Created log for partition __consumer_offsets-7 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-7 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,921] INFO [Partition __consumer_offsets-7 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,921] INFO [Partition __consumer_offsets-7 broker=1001] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,921] INFO [Partition __consumer_offsets-7 broker=1001] __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,930] INFO [Log partition=__consumer_offsets-42, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,930] INFO [Log partition=__consumer_offsets-42, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,931] INFO Created log for partition __consumer_offsets-42 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-42 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,931] INFO [Partition __consumer_offsets-42 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,931] INFO [Partition __consumer_offsets-42 broker=1001] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,931] INFO [Partition __consumer_offsets-42 broker=1001] __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,939] INFO [Log partition=__consumer_offsets-4, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,939] INFO [Log partition=__consumer_offsets-4, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,940] INFO Created log for partition __consumer_offsets-4 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,940] INFO [Partition __consumer_offsets-4 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,940] INFO [Partition __consumer_offsets-4 broker=1001] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,940] INFO [Partition __consumer_offsets-4 broker=1001] __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,949] INFO [Log partition=__consumer_offsets-23, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,949] INFO [Log partition=__consumer_offsets-23, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,950] INFO Created log for partition __consumer_offsets-23 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-23 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,950] INFO [Partition __consumer_offsets-23 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,950] INFO [Partition __consumer_offsets-23 broker=1001] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,950] INFO [Partition __consumer_offsets-23 broker=1001] __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,959] INFO [Log partition=__consumer_offsets-1, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,960] INFO [Log partition=__consumer_offsets-1, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,961] INFO Created log for partition __consumer_offsets-1 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,961] INFO [Partition __consumer_offsets-1 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,961] INFO [Partition __consumer_offsets-1 broker=1001] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,961] INFO [Partition __consumer_offsets-1 broker=1001] __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,969] INFO [Log partition=__consumer_offsets-20, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,970] INFO [Log partition=__consumer_offsets-20, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,971] INFO Created log for partition __consumer_offsets-20 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-20 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,971] INFO [Partition __consumer_offsets-20 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,971] INFO [Partition __consumer_offsets-20 broker=1001] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,971] INFO [Partition __consumer_offsets-20 broker=1001] __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,979] INFO [Log partition=__consumer_offsets-39, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,980] INFO [Log partition=__consumer_offsets-39, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,981] INFO Created log for partition __consumer_offsets-39 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-39 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,981] INFO [Partition __consumer_offsets-39 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,981] INFO [Partition __consumer_offsets-39 broker=1001] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,981] INFO [Partition __consumer_offsets-39 broker=1001] __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,990] INFO [Log partition=__consumer_offsets-17, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,990] INFO [Log partition=__consumer_offsets-17, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:13:59,991] INFO Created log for partition __consumer_offsets-17 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-17 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:13:59,991] INFO [Partition __consumer_offsets-17 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,991] INFO [Partition __consumer_offsets-17 broker=1001] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:13:59,991] INFO [Partition __consumer_offsets-17 broker=1001] __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,002] INFO [Log partition=__consumer_offsets-36, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,003] INFO [Log partition=__consumer_offsets-36, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,004] INFO Created log for partition __consumer_offsets-36 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-36 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,004] INFO [Partition __consumer_offsets-36 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,004] INFO [Partition __consumer_offsets-36 broker=1001] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,005] INFO [Partition __consumer_offsets-36 broker=1001] __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,026] INFO [Log partition=__consumer_offsets-14, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,027] INFO [Log partition=__consumer_offsets-14, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,028] INFO Created log for partition __consumer_offsets-14 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-14 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,028] INFO [Partition __consumer_offsets-14 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,029] INFO [Partition __consumer_offsets-14 broker=1001] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,029] INFO [Partition __consumer_offsets-14 broker=1001] __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,049] INFO [Log partition=__consumer_offsets-33, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,050] INFO [Log partition=__consumer_offsets-33, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,051] INFO Created log for partition __consumer_offsets-33 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-33 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,051] INFO [Partition __consumer_offsets-33 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,051] INFO [Partition __consumer_offsets-33 broker=1001] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,051] INFO [Partition __consumer_offsets-33 broker=1001] __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,069] INFO [Log partition=__consumer_offsets-49, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,069] INFO [Log partition=__consumer_offsets-49, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,071] INFO Created log for partition __consumer_offsets-49 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-49 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,071] INFO [Partition __consumer_offsets-49 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,071] INFO [Partition __consumer_offsets-49 broker=1001] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,071] INFO [Partition __consumer_offsets-49 broker=1001] __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,082] INFO [Log partition=__consumer_offsets-11, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,082] INFO [Log partition=__consumer_offsets-11, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,083] INFO Created log for partition __consumer_offsets-11 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-11 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,084] INFO [Partition __consumer_offsets-11 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,084] INFO [Partition __consumer_offsets-11 broker=1001] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,084] INFO [Partition __consumer_offsets-11 broker=1001] __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,093] INFO [Log partition=__consumer_offsets-30, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,094] INFO [Log partition=__consumer_offsets-30, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,094] INFO Created log for partition __consumer_offsets-30 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-30 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,094] INFO [Partition __consumer_offsets-30 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,095] INFO [Partition __consumer_offsets-30 broker=1001] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,095] INFO [Partition __consumer_offsets-30 broker=1001] __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,104] INFO [Log partition=__consumer_offsets-46, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,105] INFO [Log partition=__consumer_offsets-46, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,106] INFO Created log for partition __consumer_offsets-46 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-46 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,106] INFO [Partition __consumer_offsets-46 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,106] INFO [Partition __consumer_offsets-46 broker=1001] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,106] INFO [Partition __consumer_offsets-46 broker=1001] __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,115] INFO [Log partition=__consumer_offsets-27, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,115] INFO [Log partition=__consumer_offsets-27, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,116] INFO Created log for partition __consumer_offsets-27 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-27 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,116] INFO [Partition __consumer_offsets-27 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,116] INFO [Partition __consumer_offsets-27 broker=1001] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,116] INFO [Partition __consumer_offsets-27 broker=1001] __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,124] INFO [Log partition=__consumer_offsets-8, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,125] INFO [Log partition=__consumer_offsets-8, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,125] INFO Created log for partition __consumer_offsets-8 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-8 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,126] INFO [Partition __consumer_offsets-8 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,126] INFO [Partition __consumer_offsets-8 broker=1001] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,126] INFO [Partition __consumer_offsets-8 broker=1001] __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,134] INFO [Log partition=__consumer_offsets-24, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,134] INFO [Log partition=__consumer_offsets-24, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,135] INFO Created log for partition __consumer_offsets-24 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-24 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,135] INFO [Partition __consumer_offsets-24 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,135] INFO [Partition __consumer_offsets-24 broker=1001] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,135] INFO [Partition __consumer_offsets-24 broker=1001] __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,145] INFO [Log partition=__consumer_offsets-43, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,145] INFO [Log partition=__consumer_offsets-43, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,146] INFO Created log for partition __consumer_offsets-43 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-43 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,146] INFO [Partition __consumer_offsets-43 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,146] INFO [Partition __consumer_offsets-43 broker=1001] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,146] INFO [Partition __consumer_offsets-43 broker=1001] __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,155] INFO [Log partition=__consumer_offsets-5, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,155] INFO [Log partition=__consumer_offsets-5, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,156] INFO Created log for partition __consumer_offsets-5 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-5 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,156] INFO [Partition __consumer_offsets-5 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,156] INFO [Partition __consumer_offsets-5 broker=1001] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,156] INFO [Partition __consumer_offsets-5 broker=1001] __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,165] INFO [Log partition=__consumer_offsets-21, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,165] INFO [Log partition=__consumer_offsets-21, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,166] INFO Created log for partition __consumer_offsets-21 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-21 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,166] INFO [Partition __consumer_offsets-21 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,166] INFO [Partition __consumer_offsets-21 broker=1001] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,166] INFO [Partition __consumer_offsets-21 broker=1001] __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,176] INFO [Log partition=__consumer_offsets-40, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,176] INFO [Log partition=__consumer_offsets-40, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,177] INFO Created log for partition __consumer_offsets-40 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-40 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,177] INFO [Partition __consumer_offsets-40 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,177] INFO [Partition __consumer_offsets-40 broker=1001] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,177] INFO [Partition __consumer_offsets-40 broker=1001] __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,193] INFO [Log partition=__consumer_offsets-2, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,193] INFO [Log partition=__consumer_offsets-2, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,194] INFO Created log for partition __consumer_offsets-2 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,195] INFO [Partition __consumer_offsets-2 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,195] INFO [Partition __consumer_offsets-2 broker=1001] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,195] INFO [Partition __consumer_offsets-2 broker=1001] __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,204] INFO [Log partition=__consumer_offsets-37, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,204] INFO [Log partition=__consumer_offsets-37, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,205] INFO Created log for partition __consumer_offsets-37 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-37 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,205] INFO [Partition __consumer_offsets-37 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,205] INFO [Partition __consumer_offsets-37 broker=1001] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,205] INFO [Partition __consumer_offsets-37 broker=1001] __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,215] INFO [Log partition=__consumer_offsets-18, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,215] INFO [Log partition=__consumer_offsets-18, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,216] INFO Created log for partition __consumer_offsets-18 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-18 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,216] INFO [Partition __consumer_offsets-18 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,216] INFO [Partition __consumer_offsets-18 broker=1001] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,216] INFO [Partition __consumer_offsets-18 broker=1001] __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,231] INFO [Log partition=__consumer_offsets-34, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,232] INFO [Log partition=__consumer_offsets-34, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,232] INFO Created log for partition __consumer_offsets-34 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-34 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,233] INFO [Partition __consumer_offsets-34 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,233] INFO [Partition __consumer_offsets-34 broker=1001] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,233] INFO [Partition __consumer_offsets-34 broker=1001] __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,248] INFO [Log partition=__consumer_offsets-15, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,249] INFO [Log partition=__consumer_offsets-15, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,250] INFO Created log for partition __consumer_offsets-15 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-15 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,250] INFO [Partition __consumer_offsets-15 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,250] INFO [Partition __consumer_offsets-15 broker=1001] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,250] INFO [Partition __consumer_offsets-15 broker=1001] __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,259] INFO [Log partition=__consumer_offsets-12, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,259] INFO [Log partition=__consumer_offsets-12, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,260] INFO Created log for partition __consumer_offsets-12 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-12 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,260] INFO [Partition __consumer_offsets-12 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,260] INFO [Partition __consumer_offsets-12 broker=1001] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,260] INFO [Partition __consumer_offsets-12 broker=1001] __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,268] INFO [Log partition=__consumer_offsets-31, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,268] INFO [Log partition=__consumer_offsets-31, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,269] INFO Created log for partition __consumer_offsets-31 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-31 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,269] INFO [Partition __consumer_offsets-31 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,269] INFO [Partition __consumer_offsets-31 broker=1001] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,269] INFO [Partition __consumer_offsets-31 broker=1001] __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,284] INFO [Log partition=__consumer_offsets-9, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,285] INFO [Log partition=__consumer_offsets-9, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,286] INFO Created log for partition __consumer_offsets-9 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-9 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,286] INFO [Partition __consumer_offsets-9 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,286] INFO [Partition __consumer_offsets-9 broker=1001] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,286] INFO [Partition __consumer_offsets-9 broker=1001] __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,295] INFO [Log partition=__consumer_offsets-47, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,295] INFO [Log partition=__consumer_offsets-47, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,296] INFO Created log for partition __consumer_offsets-47 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-47 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,296] INFO [Partition __consumer_offsets-47 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,296] INFO [Partition __consumer_offsets-47 broker=1001] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,296] INFO [Partition __consumer_offsets-47 broker=1001] __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,304] INFO [Log partition=__consumer_offsets-19, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,305] INFO [Log partition=__consumer_offsets-19, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,305] INFO Created log for partition __consumer_offsets-19 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-19 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,306] INFO [Partition __consumer_offsets-19 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,306] INFO [Partition __consumer_offsets-19 broker=1001] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,306] INFO [Partition __consumer_offsets-19 broker=1001] __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,320] INFO [Log partition=__consumer_offsets-28, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,320] INFO [Log partition=__consumer_offsets-28, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,321] INFO Created log for partition __consumer_offsets-28 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-28 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,321] INFO [Partition __consumer_offsets-28 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,321] INFO [Partition __consumer_offsets-28 broker=1001] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,321] INFO [Partition __consumer_offsets-28 broker=1001] __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,336] INFO [Log partition=__consumer_offsets-38, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,337] INFO [Log partition=__consumer_offsets-38, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,338] INFO Created log for partition __consumer_offsets-38 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-38 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,338] INFO [Partition __consumer_offsets-38 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,338] INFO [Partition __consumer_offsets-38 broker=1001] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,338] INFO [Partition __consumer_offsets-38 broker=1001] __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,346] INFO [Log partition=__consumer_offsets-35, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,347] INFO [Log partition=__consumer_offsets-35, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,347] INFO Created log for partition __consumer_offsets-35 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-35 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,348] INFO [Partition __consumer_offsets-35 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,348] INFO [Partition __consumer_offsets-35 broker=1001] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,348] INFO [Partition __consumer_offsets-35 broker=1001] __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,356] INFO [Log partition=__consumer_offsets-6, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,356] INFO [Log partition=__consumer_offsets-6, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,357] INFO Created log for partition __consumer_offsets-6 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-6 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,357] INFO [Partition __consumer_offsets-6 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,357] INFO [Partition __consumer_offsets-6 broker=1001] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,357] INFO [Partition __consumer_offsets-6 broker=1001] __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,372] INFO [Log partition=__consumer_offsets-44, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,373] INFO [Log partition=__consumer_offsets-44, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,373] INFO Created log for partition __consumer_offsets-44 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-44 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,373] INFO [Partition __consumer_offsets-44 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,373] INFO [Partition __consumer_offsets-44 broker=1001] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,373] INFO [Partition __consumer_offsets-44 broker=1001] __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,382] INFO [Log partition=__consumer_offsets-25, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,383] INFO [Log partition=__consumer_offsets-25, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,383] INFO Created log for partition __consumer_offsets-25 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-25 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,384] INFO [Partition __consumer_offsets-25 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,384] INFO [Partition __consumer_offsets-25 broker=1001] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,384] INFO [Partition __consumer_offsets-25 broker=1001] __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,393] INFO [Log partition=__consumer_offsets-16, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,393] INFO [Log partition=__consumer_offsets-16, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,394] INFO Created log for partition __consumer_offsets-16 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-16 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,394] INFO [Partition __consumer_offsets-16 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,394] INFO [Partition __consumer_offsets-16 broker=1001] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,395] INFO [Partition __consumer_offsets-16 broker=1001] __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,403] INFO [Log partition=__consumer_offsets-22, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,404] INFO [Log partition=__consumer_offsets-22, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,404] INFO Created log for partition __consumer_offsets-22 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-22 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,404] INFO [Partition __consumer_offsets-22 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,405] INFO [Partition __consumer_offsets-22 broker=1001] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,405] INFO [Partition __consumer_offsets-22 broker=1001] __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,412] INFO [Log partition=__consumer_offsets-41, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,413] INFO [Log partition=__consumer_offsets-41, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,413] INFO Created log for partition __consumer_offsets-41 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-41 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,413] INFO [Partition __consumer_offsets-41 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,413] INFO [Partition __consumer_offsets-41 broker=1001] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,413] INFO [Partition __consumer_offsets-41 broker=1001] __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,421] INFO [Log partition=__consumer_offsets-32, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,421] INFO [Log partition=__consumer_offsets-32, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,422] INFO Created log for partition __consumer_offsets-32 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-32 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,422] INFO [Partition __consumer_offsets-32 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,422] INFO [Partition __consumer_offsets-32 broker=1001] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,422] INFO [Partition __consumer_offsets-32 broker=1001] __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,437] INFO [Log partition=__consumer_offsets-3, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,437] INFO [Log partition=__consumer_offsets-3, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,438] INFO Created log for partition __consumer_offsets-3 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,438] INFO [Partition __consumer_offsets-3 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,438] INFO [Partition __consumer_offsets-3 broker=1001] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,438] INFO [Partition __consumer_offsets-3 broker=1001] __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,448] INFO [Log partition=__consumer_offsets-13, dir=/kafka/kafka-logs-8d177c875ca7] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,449] INFO [Log partition=__consumer_offsets-13, dir=/kafka/kafka-logs-8d177c875ca7] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka_1          |[0m [2020-04-13 18:14:00,450] INFO Created log for partition __consumer_offsets-13 in /kafka/kafka-logs-8d177c875ca7/__consumer_offsets-13 with properties {compression.type -> producer, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,450] INFO [Partition __consumer_offsets-13 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,450] INFO [Partition __consumer_offsets-13 broker=1001] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,450] INFO [Partition __consumer_offsets-13 broker=1001] __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (kafka.cluster.Partition)
[36mkafka_1          |[0m [2020-04-13 18:14:00,458] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,459] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,459] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,459] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,459] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,459] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,459] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,459] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,459] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,460] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,461] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,462] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,463] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,463] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,464] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-22 in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,465] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,465] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,466] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-31 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,466] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,466] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,467] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,467] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,467] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,468] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,468] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,469] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-44 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,469] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,469] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,470] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,470] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,470] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,471] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,473] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,473] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,474] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,474] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,474] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,475] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-11 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,475] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,475] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,476] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,476] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,476] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,477] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-29 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,477] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,477] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,477] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,478] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,478] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,478] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,479] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,479] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,479] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,479] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,480] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,480] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,480] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,481] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-30 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,481] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,481] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,481] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,482] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-42 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,482] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka_1          |[0m [2020-04-13 18:14:00,482] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mnode2_1          |[0m %7|1586801640.541|CGRPQUERY|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Group "striot_con_group": querying for coordinator: intervaled in state query-coord
[35mnode2_1          |[0m %7|1586801640.541|CGRPSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed state query-coord -> wait-coord (v1, join-state init)
[35mnode2_1          |[0m %7|1586801640.541|BROADCAST|rdkafka#consumer-1| [thrd:main]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801640.541|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FindCoordinatorRequest (v2, 40 bytes @ 0, CorrId 4)
[35mnode2_1          |[0m %7|1586801640.550|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FindCoordinatorResponse (v2, 27 bytes, CorrId 4, rtt 8.51ms)
[35mnode2_1          |[0m %7|1586801640.550|CGRPCOORD|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Group "striot_con_group" coordinator is kafka:9092 id 1001
[35mnode2_1          |[0m %7|1586801640.550|CGRPCOORD|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changing coordinator -1 -> 1001
[35mnode2_1          |[0m %7|1586801640.550|COORDSET|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" coordinator set to broker kafka:9092/1001
[35mnode2_1          |[0m %7|1586801640.550|CGRPSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed state wait-coord -> wait-broker-transport (v1, join-state init)
[35mnode2_1          |[0m %7|1586801640.550|BROADCAST|rdkafka#consumer-1| [thrd:main]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801640.550|NODENAME|rdkafka#consumer-1| [thrd:main]: GroupCoordinator: Broker nodename changed from "" to "kafka:9092"
[35mnode2_1          |[0m %7|1586801640.550|CONNECT|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: Received CONNECT op
[35mnode2_1          |[0m %7|1586801640.550|STATE|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: Broker changed state INIT -> TRY_CONNECT
[35mnode2_1          |[0m %7|1586801640.550|BROADCAST|rdkafka#consumer-1| [thrd:GroupCoordinator]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801640.550|CGRPQUERY|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Group "striot_con_group": querying for coordinator: intervaled in state wait-broker-transport
[35mnode2_1          |[0m %7|1586801640.550|BROKERFAIL|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: failed: err: Local: Broker transport failure: (errno: Success)
[35mnode2_1          |[0m %7|1586801640.550|FAIL|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: Closing connection due to nodename change (after 0ms in state TRY_CONNECT)
[35mnode2_1          |[0m %7|1586801640.550|STATE|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: Broker changed state TRY_CONNECT -> DOWN
[35mnode2_1          |[0m %7|1586801640.550|BROADCAST|rdkafka#consumer-1| [thrd:GroupCoordinator]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801640.550|BUFQ|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Purging bufq with 0 buffers
[35mnode2_1          |[0m %7|1586801640.550|BUFQ|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Purging bufq with 0 buffers
[35mnode2_1          |[0m %7|1586801640.550|BUFQ|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Updating 0 buffers on connection reset
[35mnode2_1          |[0m %7|1586801640.550|STATE|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: Broker changed state DOWN -> INIT
[35mnode2_1          |[0m %7|1586801640.550|BROADCAST|rdkafka#consumer-1| [thrd:GroupCoordinator]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801640.550|STATE|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: Broker changed state INIT -> TRY_CONNECT
[35mnode2_1          |[0m %7|1586801640.550|BROADCAST|rdkafka#consumer-1| [thrd:GroupCoordinator]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801640.550|CONNECT|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: broker in state TRY_CONNECT connecting
[35mnode2_1          |[0m %7|1586801640.550|STATE|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: Broker changed state TRY_CONNECT -> CONNECT
[35mnode2_1          |[0m %7|1586801640.550|BROADCAST|rdkafka#consumer-1| [thrd:GroupCoordinator]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801640.550|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FindCoordinatorRequest (v2, 40 bytes @ 0, CorrId 5)
[35mnode2_1          |[0m %7|1586801640.550|CONNECT|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Connecting to ipv4#192.168.64.4:9092 (plaintext) with socket 11
[35mnode2_1          |[0m %7|1586801640.551|CONNECT|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Connected to ipv4#192.168.64.4:9092
[35mnode2_1          |[0m %7|1586801640.551|CONNECTED|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Connected (#1)
[35mnode2_1          |[0m %7|1586801640.551|FEATURE|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Updated enabled protocol features +ApiVersion to ApiVersion
[35mnode2_1          |[0m %7|1586801640.551|STATE|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: Broker changed state CONNECT -> APIVERSION_QUERY
[35mnode2_1          |[0m %7|1586801640.551|BROADCAST|rdkafka#consumer-1| [thrd:GroupCoordinator]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801640.551|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent ApiVersionRequest (v3, 62 bytes @ 0, CorrId 1)
[35mnode2_1          |[0m %7|1586801640.553|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received ApiVersionResponse (v3, 344 bytes, CorrId 1, rtt 2.54ms)
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Broker API support:
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey Produce (0) Versions 0..8
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey Fetch (1) Versions 0..11
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey Offset (2) Versions 0..5
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey Metadata (3) Versions 0..9
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey LeaderAndIsr (4) Versions 0..4
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey StopReplica (5) Versions 0..2
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey UpdateMetadata (6) Versions 0..6
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey ControlledShutdown (7) Versions 0..3
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey OffsetCommit (8) Versions 0..8
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey OffsetFetch (9) Versions 0..6
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey FindCoordinator (10) Versions 0..3
[35mnode2_1          |[0m %7|1586801640.553|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey JoinGroup (11) Versions 0..6
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey Heartbeat (12) Versions 0..4
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey LeaveGroup (13) Versions 0..4
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey SyncGroup (14) Versions 0..4
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey DescribeGroups (15) Versions 0..5
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey ListGroups (16) Versions 0..3
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey SaslHandshake (17) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey ApiVersion (18) Versions 0..3
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey CreateTopics (19) Versions 0..5
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey DeleteTopics (20) Versions 0..4
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey DeleteRecords (21) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey InitProducerId (22) Versions 0..2
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey OffsetForLeaderEpoch (23) Versions 0..3
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey AddPartitionsToTxn (24) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey AddOffsetsToTxn (25) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey EndTxn (26) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey WriteTxnMarkers (27) Versions 0..0
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey TxnOffsetCommit (28) Versions 0..2
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey DescribeAcls (29) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey CreateAcls (30) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey DeleteAcls (31) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey DescribeConfigs (32) Versions 0..2
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey AlterConfigs (33) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey AlterReplicaLogDirs (34) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey DescribeLogDirs (35) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey SaslAuthenticate (36) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey CreatePartitions (37) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey CreateDelegationToken (38) Versions 0..2
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey RenewDelegationToken (39) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey ExpireDelegationToken (40) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey DescribeDelegationToken (41) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey DeleteGroups (42) Versions 0..2
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey Unknown-43? (43) Versions 0..2
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey Unknown-44? (44) Versions 0..1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey Unknown-45? (45) Versions 0..0
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey Unknown-46? (46) Versions 0..0
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:   ApiKey Unknown-47? (47) Versions 0..0
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature MsgVer1: Produce (2..2) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature MsgVer1: Fetch (2..2) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature MsgVer1
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature MsgVer2: Produce (3..3) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature MsgVer2: Fetch (4..4) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature MsgVer2
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature ApiVersion: ApiVersion (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature ApiVersion
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature BrokerGroupCoordinator: FindCoordinator (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature BrokerGroupCoordinator
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature BrokerBalancedConsumer: FindCoordinator (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature BrokerBalancedConsumer: OffsetCommit (1..2) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature BrokerBalancedConsumer: OffsetFetch (1..1) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature BrokerBalancedConsumer: JoinGroup (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature BrokerBalancedConsumer: SyncGroup (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature BrokerBalancedConsumer: Heartbeat (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature BrokerBalancedConsumer: LeaveGroup (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature BrokerBalancedConsumer
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature ThrottleTime: Produce (1..2) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature ThrottleTime: Fetch (1..2) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature ThrottleTime
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature Sasl: JoinGroup (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature Sasl
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature SaslHandshake: SaslHandshake (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature SaslHandshake
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature LZ4: FindCoordinator (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature LZ4
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature OffsetTime: Offset (1..1) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature OffsetTime
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature IdempotentProducer: InitProducerId (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature IdempotentProducer
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature ZSTD: Produce (7..7) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature ZSTD: Fetch (10..10) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature ZSTD
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature SaslAuthReq: SaslHandshake (1..1) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001:  Feature SaslAuthReq: SaslAuthenticate (0..0) supported by broker
[35mnode2_1          |[0m %7|1586801640.554|APIVERSION|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Enabling feature SaslAuthReq
[35mnode2_1          |[0m %7|1586801640.554|FEATURE|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Updated enabled protocol features to MsgVer1,ApiVersion,BrokerBalancedConsumer,ThrottleTime,Sasl,SaslHandshake,BrokerGroupCoordinator,LZ4,OffsetTime,MsgVer2,IdempotentProducer,ZSTD,SaslAuthReq
[35mnode2_1          |[0m %7|1586801640.554|STATE|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator: Broker changed state APIVERSION_QUERY -> UP
[35mnode2_1          |[0m %7|1586801640.554|BROADCAST|rdkafka#consumer-1| [thrd:GroupCoordinator]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801640.554|METADATA|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Request metadata for brokers only: connected
[35mnode2_1          |[0m %7|1586801640.554|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent MetadataRequest (v2, 25 bytes @ 0, CorrId 2)
[35mnode2_1          |[0m %7|1586801640.555|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FindCoordinatorResponse (v2, 27 bytes, CorrId 5, rtt 5.01ms)
[35mnode2_1          |[0m %7|1586801640.555|CGRPCOORD|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Group "striot_con_group" coordinator is kafka:9092 id 1001
[35mnode2_1          |[0m %7|1586801640.555|CGRPSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed state wait-broker-transport -> up (v1, join-state init)
[35mnode2_1          |[0m %7|1586801640.555|BROADCAST|rdkafka#consumer-1| [thrd:main]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801640.555|JOIN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": join with 0 (1) subscribed topic(s)
[35mnode2_1          |[0m %7|1586801640.555|METADATA|rdkafka#consumer-1| [thrd:main]: Hinted cache of 1/1 topic(s) being queried
[35mnode2_1          |[0m %7|1586801640.555|CGRPMETADATA|rdkafka#consumer-1| [thrd:main]: consumer join: metadata for subscription only available for 0/1 topics (-1ms old)
[35mnode2_1          |[0m %7|1586801640.555|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Request metadata for 1 topic(s): consumer join
[35mnode2_1          |[0m %7|1586801640.555|JOIN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": postponing join until up-to-date metadata is available
[35mnode2_1          |[0m %7|1586801640.555|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received MetadataResponse (v2, 53 bytes, CorrId 2, rtt 1.27ms)
[35mnode2_1          |[0m %7|1586801640.556|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent MetadataRequest (v2, 39 bytes @ 0, CorrId 6)
[35mnode2_1          |[0m %7|1586801640.556|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: ===== Received metadata: connected =====
[35mnode2_1          |[0m %7|1586801640.556|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: ClusterId: okF80LrDSN-z5mN0v_DfCw, ControllerId: 1001
[35mnode2_1          |[0m %7|1586801640.556|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: 1 brokers, 0 topics
[35mnode2_1          |[0m %7|1586801640.556|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001:   Broker #0/1: kafka:9092 NodeId 1001
[35mnode2_1          |[0m %7|1586801640.558|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received MetadataResponse (v2, 100 bytes, CorrId 6, rtt 2.01ms)
[35mnode2_1          |[0m %7|1586801640.558|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: ===== Received metadata (for 1 requested topics): consumer join =====
[35mnode2_1          |[0m %7|1586801640.558|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: ClusterId: okF80LrDSN-z5mN0v_DfCw, ControllerId: 1001
[35mnode2_1          |[0m %7|1586801640.558|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: 1 brokers, 1 topics
[35mnode2_1          |[0m %7|1586801640.558|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001:   Broker #0/1: kafka:9092 NodeId 1001
[35mnode2_1          |[0m %7|1586801640.558|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001:   Topic #0/1: striot-queue with 1 partitions
[35mnode2_1          |[0m %7|1586801640.558|METADATA|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: 1/1 requested topic(s) seen in metadata
[35mnode2_1          |[0m %7|1586801640.558|SUBSCRIPTION|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": effective subscription list changed from 0 to 1 topic(s):
[35mnode2_1          |[0m %7|1586801640.558|SUBSCRIPTION|rdkafka#consumer-1| [thrd:main]:  Topic striot-queue with 1 partition(s)
[35mnode2_1          |[0m %7|1586801640.558|REJOIN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": subscription updated from metadata change: rejoining group
[35mnode2_1          |[0m %7|1586801640.558|GRPLEADER|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": resetting group leader info: Group rejoin
[35mnode2_1          |[0m %7|1586801640.558|REJOIN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" rejoining in join-state init without an assignment
[35mnode2_1          |[0m %7|1586801640.558|REBALANCE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" is rebalancing in state up (join-state init) without assignment: group rejoin
[35mnode2_1          |[0m %7|1586801640.558|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state init -> wait-unassign (v1, state up)
[35mnode2_1          |[0m %7|1586801640.558|UNASSIGN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": unassign done in state up (join state wait-unassign): without new assignment: unassign (no previous assignment)
[35mnode2_1          |[0m %7|1586801640.558|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state wait-unassign -> init (v1, state up)
[35mnode2_1          |[0m %7|1586801641.541|COMMIT|rdkafka#consumer-1| [thrd:main]: OffsetCommit for -1 partition(s): cgrp auto commit timer: returned: Local: No offset stored
[35mnode2_1          |[0m %7|1586801641.541|UNASSIGN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": unassign done in state up (join state init): without new assignment: OffsetCommit done (__NO_OFFSET)
[35mnode2_1          |[0m %7|1586801642.540|JOIN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": join with 1 (1) subscribed topic(s)
[35mnode2_1          |[0m %7|1586801642.540|CGRPMETADATA|rdkafka#consumer-1| [thrd:main]: consumer join: metadata for subscription is up to date (1982ms old)
[35mnode2_1          |[0m %7|1586801642.540|JOIN|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Joining group "striot_con_group" with 1 subscribed topic(s)
[35mnode2_1          |[0m %7|1586801642.540|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state init -> wait-join (v1, state up)
[35mnode2_1          |[0m %7|1586801642.540|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent JoinGroupRequest (v5, 140 bytes @ 0, CorrId 3)
[35mnode2_1          |[0m %7|1586801642.540|BROADCAST|rdkafka#consumer-1| [thrd:GroupCoordinator]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801642.573|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received JoinGroupResponse (v5, 64 bytes, CorrId 3, rtt 32.75ms)
[35mnode2_1          |[0m %7|1586801642.573|JOINGROUP|rdkafka#consumer-1| [thrd:main]: JoinGroup response: GenerationId -1, Protocol , LeaderId , my MemberId rdkafka-5a9ec489-978b-4afa-99ac-c9239c59b2ac, 0 members in group: Broker: Group member needs a valid member ID
[35mnode2_1          |[0m %7|1586801642.573|REQERR|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: JoinGroupRequest failed: Broker: Group member needs a valid member ID: explicit actions Ignore
[35mnode2_1          |[0m %7|1586801642.573|MEMBERID|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": updating member id "" -> "rdkafka-5a9ec489-978b-4afa-99ac-c9239c59b2ac"
[35mnode2_1          |[0m %7|1586801642.573|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state wait-join -> init (v1, state up)
[35mnode2_1          |[0m %7|1586801642.573|JOIN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": join with 1 (1) subscribed topic(s)
[35mnode2_1          |[0m %7|1586801642.573|CGRPMETADATA|rdkafka#consumer-1| [thrd:main]: consumer join: metadata for subscription is up to date (2015ms old)
[35mnode2_1          |[0m %7|1586801642.573|JOIN|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Joining group "striot_con_group" with 1 subscribed topic(s)
[35mnode2_1          |[0m %7|1586801642.573|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state init -> wait-join (v1, state up)
[35mnode2_1          |[0m %7|1586801642.573|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent JoinGroupRequest (v5, 184 bytes @ 0, CorrId 4)
[35mnode2_1          |[0m %7|1586801642.573|BROADCAST|rdkafka#consumer-1| [thrd:GroupCoordinator]: Broadcasting state change
[36mkafka_1          |[0m [2020-04-13 18:14:02,581] INFO [GroupCoordinator 1001]: Preparing to rebalance group striot_con_group in state PreparingRebalance with old generation 0 (__consumer_offsets-20) (reason: Adding new member rdkafka-5a9ec489-978b-4afa-99ac-c9239c59b2ac with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[36mkafka_1          |[0m [2020-04-13 18:14:02,590] INFO [GroupCoordinator 1001]: Stabilized group striot_con_group generation 1 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[35mnode2_1          |[0m %7|1586801642.593|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received JoinGroupResponse (v5, 189 bytes, CorrId 4, rtt 19.42ms)
[35mnode2_1          |[0m %7|1586801642.593|JOINGROUP|rdkafka#consumer-1| [thrd:main]: JoinGroup response: GenerationId 1, Protocol range, LeaderId rdkafka-5a9ec489-978b-4afa-99ac-c9239c59b2ac (me), my MemberId rdkafka-5a9ec489-978b-4afa-99ac-c9239c59b2ac, 1 members in group: (no error)
[35mnode2_1          |[0m %7|1586801642.593|JOINGROUP|rdkafka#consumer-1| [thrd:main]: Elected leader for group "striot_con_group" with 1 member(s)
[35mnode2_1          |[0m %7|1586801642.593|GRPLEADER|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": resetting group leader info: JoinGroup response clean-up
[35mnode2_1          |[0m %7|1586801642.593|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state wait-join -> wait-metadata (v1, state up)
[35mnode2_1          |[0m %7|1586801642.593|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Request metadata for 1 topic(s): partition assignor
[35mnode2_1          |[0m %7|1586801642.593|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent MetadataRequest (v2, 39 bytes @ 0, CorrId 5)
[35mnode2_1          |[0m %7|1586801642.595|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received MetadataResponse (v2, 100 bytes, CorrId 5, rtt 1.65ms)
[35mnode2_1          |[0m %7|1586801642.595|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: ===== Received metadata (for 1 requested topics): partition assignor =====
[35mnode2_1          |[0m %7|1586801642.595|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: ClusterId: okF80LrDSN-z5mN0v_DfCw, ControllerId: 1001
[35mnode2_1          |[0m %7|1586801642.595|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: 1 brokers, 1 topics
[35mnode2_1          |[0m %7|1586801642.595|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001:   Broker #0/1: kafka:9092 NodeId 1001
[35mnode2_1          |[0m %7|1586801642.595|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001:   Topic #0/1: striot-queue with 1 partitions
[35mnode2_1          |[0m %7|1586801642.595|METADATA|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: 1/1 requested topic(s) seen in metadata
[35mnode2_1          |[0m %7|1586801642.595|ASSIGN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" running range assignment for 1 member(s):
[35mnode2_1          |[0m %7|1586801642.595|ASSIGN|rdkafka#consumer-1| [thrd:main]:  Member "rdkafka-5a9ec489-978b-4afa-99ac-c9239c59b2ac" (me) with 1 subscription(s):
[35mnode2_1          |[0m %7|1586801642.595|ASSIGN|rdkafka#consumer-1| [thrd:main]:   striot-queue [-1]
[35mnode2_1          |[0m %7|1586801642.595|ASSIGN|rdkafka#consumer-1| [thrd:main]: range: Topic striot-queue with 1 partition(s) and 1 subscribing member(s)
[35mnode2_1          |[0m %7|1586801642.595|ASSIGN|rdkafka#consumer-1| [thrd:main]: range: Member "rdkafka-5a9ec489-978b-4afa-99ac-c9239c59b2ac": assigned topic striot-queue partitions 0..0
[35mnode2_1          |[0m %7|1586801642.595|ASSIGN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" range assignment for 1 member(s) finished in 0.041ms:
[35mnode2_1          |[0m %7|1586801642.595|ASSIGN|rdkafka#consumer-1| [thrd:main]:  Member "rdkafka-5a9ec489-978b-4afa-99ac-c9239c59b2ac" (me) assigned 1 partition(s):
[35mnode2_1          |[0m %7|1586801642.595|ASSIGN|rdkafka#consumer-1| [thrd:main]:   striot-queue [0]
[35mnode2_1          |[0m %7|1586801642.595|ASSIGNOR|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": "range" assignor run for 1 member(s)
[35mnode2_1          |[0m %7|1586801642.595|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state wait-metadata -> wait-sync (v1, state up)
[35mnode2_1          |[0m %7|1586801642.595|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent SyncGroupRequest (v3, 177 bytes @ 0, CorrId 6)
[35mnode2_1          |[0m %7|1586801642.595|BROADCAST|rdkafka#consumer-1| [thrd:GroupCoordinator]: Broadcasting state change
[36mkafka_1          |[0m [2020-04-13 18:14:02,598] INFO [GroupCoordinator 1001]: Assignment received from leader for group striot_con_group for generation 1 (kafka.coordinator.group.GroupCoordinator)
[35mnode2_1          |[0m %7|1586801642.615|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received SyncGroupResponse (v3, 42 bytes, CorrId 6, rtt 19.65ms)
[35mnode2_1          |[0m %7|1586801642.615|SYNCGROUP|rdkafka#consumer-1| [thrd:main]: SyncGroup response: Success (32 bytes of MemberState data)
[35mnode2_1          |[0m %7|1586801642.615|ASSIGN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": new assignment of 1 partition(s) in join state wait-sync
[35mnode2_1          |[0m %7|1586801642.615|TOPIC|rdkafka#consumer-1| [thrd:main]: New local topic: striot-queue
[35mnode2_1          |[0m %7|1586801642.615|TOPPARNEW|rdkafka#consumer-1| [thrd:main]: NEW striot-queue [-1] 0x7fb854001980 (at rd_kafka_topic_new0:397)
[35mnode2_1          |[0m %7|1586801642.615|STATE|rdkafka#consumer-1| [thrd:main]: Topic striot-queue changed state unknown -> exists
[35mnode2_1          |[0m %7|1586801642.615|PARTCNT|rdkafka#consumer-1| [thrd:main]: Topic striot-queue partition count changed from 0 to 1
[35mnode2_1          |[0m %7|1586801642.615|TOPPARNEW|rdkafka#consumer-1| [thrd:main]: NEW striot-queue [0] 0x7fb854001f00 (at rd_kafka_topic_partition_cnt_update:708)
[35mnode2_1          |[0m %7|1586801642.615|METADATA|rdkafka#consumer-1| [thrd:main]:   Topic striot-queue partition 0 Leader 1001
[35mnode2_1          |[0m %7|1586801642.615|BRKDELGT|rdkafka#consumer-1| [thrd:main]: striot-queue [0]: delegate to broker kafka:9092/1001 (rktp 0x7fb854001f00, term 0, ref 2)
[35mnode2_1          |[0m %7|1586801642.615|BRKDELGT|rdkafka#consumer-1| [thrd:main]: striot-queue [0]: delegating to broker kafka:9092/1001 for partition with 0 messages (0 bytes) queued
[35mnode2_1          |[0m %7|1586801642.615|BRKMIGR|rdkafka#consumer-1| [thrd:main]: Migrating topic striot-queue [0] 0x7fb854001f00 from (none) to kafka:9092/1001 (sending PARTITION_JOIN to kafka:9092/1001)
[35mnode2_1          |[0m %7|1586801642.615|DESP|rdkafka#consumer-1| [thrd:main]: Setting topic striot-queue [0] partition as desired
[35mnode2_1          |[0m %7|1586801642.615|BARRIER|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": rd_kafka_cgrp_assign:2563: new version barrier v2
[35mnode2_1          |[0m %7|1586801642.615|ASSIGN|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": assigning 1 partition(s) in join state wait-sync
[35mnode2_1          |[0m %7|1586801642.615|TOPBRK|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0]: joining broker (rktp 0x7fb854001f00, 0 message(s) queued)
[35mnode2_1          |[0m %7|1586801642.615|BROADCAST|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Broadcasting state change
[35mnode2_1          |[0m %7|1586801642.615|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state wait-sync -> assigned (v2, state up)
[35mnode2_1          |[0m %7|1586801642.615|BARRIER|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": rd_kafka_cgrp_partitions_fetch_start0:1848: new version barrier v3
[35mnode2_1          |[0m %7|1586801642.615|FETCHSTART|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": starting fetchers for 1 assigned partition(s) in join-state assigned (usable_offsets=no, v3, line 2616)
[35mnode2_1          |[0m %7|1586801642.615|FETCHSTART|rdkafka#consumer-1| [thrd:main]: List with 1 partition(s):
[35mnode2_1          |[0m %7|1586801642.615|FETCHSTART|rdkafka#consumer-1| [thrd:main]:  striot-queue [0] offset INVALID
[35mnode2_1          |[0m %7|1586801642.615|OFFSET|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: OffsetFetchRequest(v1) for 1/1 partition(s)
[35mnode2_1          |[0m %7|1586801642.615|OFFSET|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Fetch committed offsets for 1/1 partition(s)
[35mnode2_1          |[0m %7|1586801642.615|HEARTBEAT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Heartbeat for group "striot_con_group" generation id 1
[35mnode2_1          |[0m %7|1586801642.615|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent OffsetFetchRequest (v1, 65 bytes @ 0, CorrId 7)
[35mnode2_1          |[0m %7|1586801642.615|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent HeartbeatRequest (v3, 91 bytes @ 0, CorrId 8)
[35mnode2_1          |[0m %7|1586801642.623|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received OffsetFetchResponse (v1, 38 bytes, CorrId 7, rtt 7.48ms)
[35mnode2_1          |[0m %7|1586801642.623|OFFSETFETCH|rdkafka#consumer-1| [thrd:main]: List with 1 partition(s):
[35mnode2_1          |[0m %7|1586801642.623|OFFSETFETCH|rdkafka#consumer-1| [thrd:main]:  striot-queue [0] offset INVALID
[35mnode2_1          |[0m %7|1586801642.623|OFFSET|rdkafka#consumer-1| [thrd:main]: Topic striot-queue [0]: setting default offset INVALID
[35mnode2_1          |[0m %7|1586801642.623|OFFSETFETCH|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: OffsetFetchResponse: striot-queue [0] offset -1, metadata 0 byte(s)
[35mnode2_1          |[0m %7|1586801642.623|OFFFETCH|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: OffsetFetch for 1/1 partition(s) returned Success
[35mnode2_1          |[0m %7|1586801642.623|BARRIER|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": rd_kafka_cgrp_partitions_fetch_start0:1848: new version barrier v4
[35mnode2_1          |[0m %7|1586801642.623|FETCHSTART|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": starting fetchers for 1 assigned partition(s) in join-state assigned (usable_offsets=yes, v4, line 1780)
[35mnode2_1          |[0m %7|1586801642.623|FETCHSTART|rdkafka#consumer-1| [thrd:main]: List with 1 partition(s):
[35mnode2_1          |[0m %7|1586801642.623|FETCHSTART|rdkafka#consumer-1| [thrd:main]:  striot-queue [0] offset INVALID
[35mnode2_1          |[0m %7|1586801642.623|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" changed join state assigned -> started (v4, state up)
[35mnode2_1          |[0m %7|1586801642.623|BARRIER|rdkafka#consumer-1| [thrd:main]: striot-queue [0]: rd_kafka_toppar_op_fetch_start:2307: new version barrier v2
[35mnode2_1          |[0m %7|1586801642.623|CONSUMER|rdkafka#consumer-1| [thrd:main]: Start consuming striot-queue [0] at offset INVALID (v2)
[35mnode2_1          |[0m %7|1586801642.623|OP|rdkafka#consumer-1| [thrd:main]: striot-queue [0] received op FETCH_START (v2) in fetch-state none (opv1)
[35mnode2_1          |[0m %7|1586801642.623|FETCH|rdkafka#consumer-1| [thrd:main]: Start fetch for striot-queue [0] in state none at offset INVALID (v2)
[35mnode2_1          |[0m %7|1586801642.623|PARTSTATE|rdkafka#consumer-1| [thrd:main]: Partition striot-queue [0] changed fetch state none -> offset-query
[35mnode2_1          |[0m %7|1586801642.623|OFFSET|rdkafka#consumer-1| [thrd:main]: striot-queue [0]: offset reset (at offset INVALID) to BEGINNING: no previously committed offset available: Local: No offset stored
[35mnode2_1          |[0m %7|1586801642.623|OFFREQ|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Partition striot-queue [0]: querying for logical offset BEGINNING (opv 2)
[35mnode2_1          |[0m %7|1586801642.623|OFFSET|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: OffsetRequest (v0, opv 0) for 1 topic(s) and 1 partition(s)
[35mnode2_1          |[0m %7|1586801642.623|PARTSTATE|rdkafka#consumer-1| [thrd:main]: Partition striot-queue [0] changed fetch state offset-query -> offset-wait
[35mnode2_1          |[0m %7|1586801642.623|CGRPOP|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group" received op PARTITION_JOIN in state up (join state started, v4) for striot-queue [0]
[35mnode2_1          |[0m %7|1586801642.623|PARTADD|rdkafka#consumer-1| [thrd:main]: Group "striot_con_group": add striot-queue [0]
[35mnode2_1          |[0m %7|1586801642.623|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent OffsetRequest (v0, 63 bytes @ 0, CorrId 7)
[35mnode2_1          |[0m %7|1586801642.628|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received HeartbeatResponse (v3, 6 bytes, CorrId 8, rtt 12.99ms)
[35mnode2_1          |[0m %7|1586801642.639|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received OffsetResponse (v0, 40 bytes, CorrId 7, rtt 15.67ms)
[35mnode2_1          |[0m %7|1586801642.639|OFFSET|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Offset reply for topic striot-queue [0] (v2 vs v2)
[35mnode2_1          |[0m %7|1586801642.639|OFFSET|rdkafka#consumer-1| [thrd:main]: Offset BEGINNING request for striot-queue [0] returned offset 0 (0)
[35mnode2_1          |[0m %7|1586801642.639|PARTSTATE|rdkafka#consumer-1| [thrd:main]: Partition striot-queue [0] changed fetch state offset-wait -> active
[35mnode2_1          |[0m %7|1586801642.639|FETCH|rdkafka#consumer-1| [thrd:main]: Partition striot-queue [0] start fetching at offset 0
[35mnode2_1          |[0m %7|1586801642.639|WAKEUP|rdkafka#consumer-1| [thrd:main]: kafka:9092/1001: Wake-up
[35mnode2_1          |[0m %7|1586801642.639|FETCHDEC|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: Topic striot-queue [0]: fetch decide: updating to version 2 (was 0) at offset 0 (was 0)
[35mnode2_1          |[0m %7|1586801642.639|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] in state active at offset 0 (0/100000 msgs, 0/1048576 kb queued, opv 2) is fetchable
[35mnode2_1          |[0m %7|1586801642.639|FETCHADD|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Added striot-queue [0] to fetch list (1 entries, opv 2, 0 messages queued): fetchable
[35mnode2_1          |[0m %7|1586801642.639|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 0 (v2)
[35mnode2_1          |[0m %7|1586801642.639|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801642.639|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 8)
[35mnode2_1          |[0m %7|1586801642.671|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 1269 bytes, CorrId 8, rtt 32.03ms)
[35mnode2_1          |[0m %7|1586801642.671|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 1195, error "Success", MaxOffset 5, LSO 5, Ver 2/2
[35mnode2_1          |[0m %7|1586801642.671|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 5 message(s) (905 bytes, 5 ops) on striot-queue [0] fetch queue (qlen 5, v2, last_offset 4, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801642.671|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 5 (v2)
[35mnode2_1          |[0m %7|1586801642.671|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801642.671|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 9)
[35mnode2_1          |[0m %7|1586801642.781|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 9, rtt 110.05ms)
[35mnode2_1          |[0m %7|1586801642.781|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 5, LSO 5, Ver 2/2
[35mnode2_1          |[0m %7|1586801642.781|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 5 (v2)
[35mnode2_1          |[0m %7|1586801642.781|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801642.781|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 10)
[35mnode2_1          |[0m "{4}: Send stream"
[35mnode2_1          |[0m %7|1586801642.884|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 10, rtt 102.82ms)
[35mnode2_1          |[0m %7|1586801642.884|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 5, LSO 5, Ver 2/2
[35mnode2_1          |[0m %7|1586801642.884|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 5 (v2)
[35mnode2_1          |[0m %7|1586801642.884|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801642.884|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 11)
[35mnode2_1          |[0m %7|1586801642.988|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 11, rtt 103.77ms)
[35mnode2_1          |[0m %7|1586801642.988|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 5, LSO 5, Ver 2/2
[35mnode2_1          |[0m %7|1586801642.988|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 5 (v2)
[35mnode2_1          |[0m %7|1586801642.988|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801642.988|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 12)
[35mnode2_1          |[0m %7|1586801643.092|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 12, rtt 103.47ms)
[35mnode2_1          |[0m %7|1586801643.092|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 5, LSO 5, Ver 2/2
[35mnode2_1          |[0m %7|1586801643.092|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 5 (v2)
[35mnode2_1          |[0m %7|1586801643.092|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801643.092|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 13)
[34mnode3_1          |[0m Event {eventId = 0, manage = Nothing, time = Just 2020-04-13 18:13:57.254087731 UTC, value = Just "Incoming Message at Server: 0"}
[35mnode2_1          |[0m %7|1586801643.195|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 13, rtt 103.28ms)
[35mnode2_1          |[0m %7|1586801643.195|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 5, LSO 5, Ver 2/2
[35mnode2_1          |[0m %7|1586801643.195|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 5 (v2)
[35mnode2_1          |[0m %7|1586801643.195|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801643.195|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 14)
[35mnode2_1          |[0m %7|1586801643.267|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 14, rtt 71.29ms)
[35mnode2_1          |[0m %7|1586801643.267|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 6, LSO 6, Ver 2/2
[35mnode2_1          |[0m %7|1586801643.267|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 5, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801643.267|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 6 (v2)
[35mnode2_1          |[0m %7|1586801643.267|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801643.267|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 15)
[35mnode2_1          |[0m %7|1586801643.371|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 15, rtt 104.02ms)
[35mnode2_1          |[0m %7|1586801643.371|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 6, LSO 6, Ver 2/2
[35mnode2_1          |[0m %7|1586801643.371|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 6 (v2)
[35mnode2_1          |[0m %7|1586801643.371|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801643.371|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 16)
[34mnode3_1          |[0m Event {eventId = 1, manage = Nothing, time = Just 2020-04-13 18:13:58.255264768 UTC, value = Just "Incoming Message at Server: 1"}
[35mnode2_1          |[0m %7|1586801643.474|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 16, rtt 103.52ms)
[35mnode2_1          |[0m %7|1586801643.474|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 6, LSO 6, Ver 2/2
[35mnode2_1          |[0m %7|1586801643.474|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 6 (v2)
[35mnode2_1          |[0m %7|1586801643.475|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801643.475|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 17)
[35mnode2_1          |[0m %7|1586801643.578|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 17, rtt 102.96ms)
[35mnode2_1          |[0m %7|1586801643.578|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 6, LSO 6, Ver 2/2
[35mnode2_1          |[0m %7|1586801643.578|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 6 (v2)
[35mnode2_1          |[0m %7|1586801643.578|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801643.578|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 18)
[35mnode2_1          |[0m %7|1586801643.680|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 18, rtt 102.48ms)
[35mnode2_1          |[0m %7|1586801643.680|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 6, LSO 6, Ver 2/2
[35mnode2_1          |[0m %7|1586801643.680|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 6 (v2)
[35mnode2_1          |[0m %7|1586801643.680|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801643.680|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 19)
[34mnode3_1          |[0m Event {eventId = 2, manage = Nothing, time = Just 2020-04-13 18:13:59.256552767 UTC, value = Just "Incoming Message at Server: 2"}
[35mnode2_1          |[0m %7|1586801643.783|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 19, rtt 102.71ms)
[35mnode2_1          |[0m %7|1586801643.783|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 6, LSO 6, Ver 2/2
[35mnode2_1          |[0m %7|1586801643.783|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 6 (v2)
[35mnode2_1          |[0m %7|1586801643.783|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801643.783|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 20)
[35mnode2_1          |[0m %7|1586801643.886|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 20, rtt 102.63ms)
[35mnode2_1          |[0m %7|1586801643.886|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 6, LSO 6, Ver 2/2
[35mnode2_1          |[0m %7|1586801643.886|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 6 (v2)
[35mnode2_1          |[0m %7|1586801643.886|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801643.886|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 21)
[35mnode2_1          |[0m %7|1586801643.990|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 21, rtt 103.76ms)
[35mnode2_1          |[0m %7|1586801643.990|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 6, LSO 6, Ver 2/2
[35mnode2_1          |[0m %7|1586801643.990|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 6 (v2)
[35mnode2_1          |[0m %7|1586801643.990|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801643.990|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 22)
[34mnode3_1          |[0m Event {eventId = 3, manage = Nothing, time = Just 2020-04-13 18:14:00.257806635 UTC, value = Just "Incoming Message at Server: 3"}
[35mnode2_1          |[0m %7|1586801644.093|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 22, rtt 103.40ms)
[35mnode2_1          |[0m %7|1586801644.093|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 6, LSO 6, Ver 2/2
[35mnode2_1          |[0m %7|1586801644.093|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 6 (v2)
[35mnode2_1          |[0m %7|1586801644.093|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801644.093|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 23)
[35mnode2_1          |[0m %7|1586801644.196|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 23, rtt 103.00ms)
[35mnode2_1          |[0m %7|1586801644.196|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 6, LSO 6, Ver 2/2
[35mnode2_1          |[0m %7|1586801644.197|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 6 (v2)
[35mnode2_1          |[0m %7|1586801644.197|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801644.197|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 24)
[35mnode2_1          |[0m %7|1586801644.269|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 24, rtt 72.00ms)
[35mnode2_1          |[0m %7|1586801644.269|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 7, LSO 7, Ver 2/2
[35mnode2_1          |[0m %7|1586801644.269|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 6, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801644.269|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 7 (v2)
[35mnode2_1          |[0m %7|1586801644.269|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801644.269|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 25)
[34mnode3_1          |[0m Event {eventId = 4, manage = Nothing, time = Just 2020-04-13 18:14:01.258664554 UTC, value = Just "Incoming Message at Server: 4"}
[35mnode2_1          |[0m %7|1586801644.372|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 25, rtt 103.11ms)
[35mnode2_1          |[0m %7|1586801644.372|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 7, LSO 7, Ver 2/2
[35mnode2_1          |[0m %7|1586801644.372|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 7 (v2)
[35mnode2_1          |[0m %7|1586801644.372|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801644.372|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 26)
[35mnode2_1          |[0m %7|1586801644.475|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 26, rtt 103.28ms)
[35mnode2_1          |[0m %7|1586801644.475|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 7, LSO 7, Ver 2/2
[35mnode2_1          |[0m %7|1586801644.475|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 7 (v2)
[35mnode2_1          |[0m %7|1586801644.475|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801644.475|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 27)
[35mnode2_1          |[0m %7|1586801644.578|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 27, rtt 103.03ms)
[35mnode2_1          |[0m %7|1586801644.579|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 7, LSO 7, Ver 2/2
[35mnode2_1          |[0m %7|1586801644.579|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 7 (v2)
[35mnode2_1          |[0m %7|1586801644.579|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801644.579|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 28)
[34mnode3_1          |[0m Event {eventId = 5, manage = Nothing, time = Just 2020-04-13 18:14:02.26003118 UTC, value = Just "Incoming Message at Server: 5"}
[35mnode2_1          |[0m %7|1586801644.683|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 28, rtt 104.48ms)
[35mnode2_1          |[0m %7|1586801644.683|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 7, LSO 7, Ver 2/2
[35mnode2_1          |[0m %7|1586801644.683|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 7 (v2)
[35mnode2_1          |[0m %7|1586801644.683|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801644.683|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 29)
[35mnode2_1          |[0m %7|1586801644.786|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 29, rtt 102.69ms)
[35mnode2_1          |[0m %7|1586801644.786|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 7, LSO 7, Ver 2/2
[35mnode2_1          |[0m %7|1586801644.786|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 7 (v2)
[35mnode2_1          |[0m %7|1586801644.786|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801644.786|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 30)
[35mnode2_1          |[0m %7|1586801644.889|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 30, rtt 102.94ms)
[35mnode2_1          |[0m %7|1586801644.889|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 7, LSO 7, Ver 2/2
[35mnode2_1          |[0m %7|1586801644.889|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 7 (v2)
[35mnode2_1          |[0m %7|1586801644.889|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801644.889|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 31)
[35mnode2_1          |[0m %7|1586801644.992|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 31, rtt 103.03ms)
[35mnode2_1          |[0m %7|1586801644.992|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 7, LSO 7, Ver 2/2
[35mnode2_1          |[0m %7|1586801644.992|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 7 (v2)
[35mnode2_1          |[0m %7|1586801644.992|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801644.992|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 32)
[35mnode2_1          |[0m %7|1586801645.095|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 32, rtt 102.90ms)
[35mnode2_1          |[0m %7|1586801645.095|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 7, LSO 7, Ver 2/2
[35mnode2_1          |[0m %7|1586801645.095|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 7 (v2)
[35mnode2_1          |[0m %7|1586801645.095|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801645.095|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 33)
[35mnode2_1          |[0m %7|1586801645.199|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 33, rtt 103.13ms)
[35mnode2_1          |[0m %7|1586801645.199|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 7, LSO 7, Ver 2/2
[35mnode2_1          |[0m %7|1586801645.199|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 7 (v2)
[35mnode2_1          |[0m %7|1586801645.199|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801645.199|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 34)
[35mnode2_1          |[0m %7|1586801645.270|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 34, rtt 71.35ms)
[35mnode2_1          |[0m %7|1586801645.270|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 8, LSO 8, Ver 2/2
[35mnode2_1          |[0m %7|1586801645.270|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 7, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801645.270|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 8 (v2)
[35mnode2_1          |[0m %7|1586801645.270|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801645.270|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 35)
[34mnode3_1          |[0m Event {eventId = 6, manage = Nothing, time = Just 2020-04-13 18:14:03.261364008 UTC, value = Just "Incoming Message at Server: 6"}
[35mnode2_1          |[0m %7|1586801645.373|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 35, rtt 102.75ms)
[35mnode2_1          |[0m %7|1586801645.373|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 8, LSO 8, Ver 2/2
[35mnode2_1          |[0m %7|1586801645.373|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 8 (v2)
[35mnode2_1          |[0m %7|1586801645.373|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801645.373|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 36)
[35mnode2_1          |[0m %7|1586801645.476|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 36, rtt 102.70ms)
[35mnode2_1          |[0m %7|1586801645.476|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 8, LSO 8, Ver 2/2
[35mnode2_1          |[0m %7|1586801645.476|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 8 (v2)
[35mnode2_1          |[0m %7|1586801645.476|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801645.476|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 37)
[35mnode2_1          |[0m %7|1586801645.580|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 37, rtt 103.95ms)
[35mnode2_1          |[0m %7|1586801645.580|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 8, LSO 8, Ver 2/2
[35mnode2_1          |[0m %7|1586801645.580|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 8 (v2)
[35mnode2_1          |[0m %7|1586801645.580|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801645.580|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 38)
[35mnode2_1          |[0m %7|1586801645.622|HEARTBEAT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Heartbeat for group "striot_con_group" generation id 1
[35mnode2_1          |[0m %7|1586801645.622|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent HeartbeatRequest (v3, 91 bytes @ 0, CorrId 9)
[35mnode2_1          |[0m %7|1586801645.625|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received HeartbeatResponse (v3, 6 bytes, CorrId 9, rtt 2.34ms)
[35mnode2_1          |[0m %7|1586801645.683|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 38, rtt 103.23ms)
[35mnode2_1          |[0m %7|1586801645.683|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 8, LSO 8, Ver 2/2
[35mnode2_1          |[0m %7|1586801645.683|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 8 (v2)
[35mnode2_1          |[0m %7|1586801645.683|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801645.683|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 39)
[35mnode2_1          |[0m %7|1586801645.788|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 39, rtt 104.59ms)
[35mnode2_1          |[0m %7|1586801645.788|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 8, LSO 8, Ver 2/2
[35mnode2_1          |[0m %7|1586801645.788|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 8 (v2)
[35mnode2_1          |[0m %7|1586801645.788|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801645.788|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 40)
[36mkafka_1          |[0m creating topics: striot-queue:1:1
[35mnode2_1          |[0m %7|1586801645.891|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 40, rtt 103.20ms)
[35mnode2_1          |[0m %7|1586801645.891|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 8, LSO 8, Ver 2/2
[35mnode2_1          |[0m %7|1586801645.892|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 8 (v2)
[35mnode2_1          |[0m %7|1586801645.892|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801645.892|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 41)
[35mnode2_1          |[0m %7|1586801645.995|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 41, rtt 102.97ms)
[35mnode2_1          |[0m %7|1586801645.995|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 8, LSO 8, Ver 2/2
[35mnode2_1          |[0m %7|1586801645.995|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 8 (v2)
[35mnode2_1          |[0m %7|1586801645.995|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801645.995|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 42)
[35mnode2_1          |[0m %7|1586801646.099|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 42, rtt 104.04ms)
[35mnode2_1          |[0m %7|1586801646.099|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 8, LSO 8, Ver 2/2
[35mnode2_1          |[0m %7|1586801646.099|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 8 (v2)
[35mnode2_1          |[0m %7|1586801646.099|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801646.099|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 43)
[35mnode2_1          |[0m %7|1586801646.201|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 43, rtt 102.51ms)
[35mnode2_1          |[0m %7|1586801646.201|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 8, LSO 8, Ver 2/2
[35mnode2_1          |[0m %7|1586801646.202|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 8 (v2)
[35mnode2_1          |[0m %7|1586801646.202|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801646.202|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 44)
[35mnode2_1          |[0m %7|1586801646.270|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 44, rtt 68.71ms)
[35mnode2_1          |[0m %7|1586801646.270|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 9, LSO 9, Ver 2/2
[35mnode2_1          |[0m %7|1586801646.270|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 8, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801646.270|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 9 (v2)
[35mnode2_1          |[0m %7|1586801646.270|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801646.270|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 45)
[34mnode3_1          |[0m Event {eventId = 7, manage = Nothing, time = Just 2020-04-13 18:14:04.26268957 UTC, value = Just "Incoming Message at Server: 7"}
[35mnode2_1          |[0m %7|1586801646.374|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 45, rtt 103.17ms)
[35mnode2_1          |[0m %7|1586801646.374|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 9, LSO 9, Ver 2/2
[35mnode2_1          |[0m %7|1586801646.374|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 9 (v2)
[35mnode2_1          |[0m %7|1586801646.374|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801646.374|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 46)
[35mnode2_1          |[0m %7|1586801646.476|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 46, rtt 102.30ms)
[35mnode2_1          |[0m %7|1586801646.476|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 9, LSO 9, Ver 2/2
[35mnode2_1          |[0m %7|1586801646.476|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 9 (v2)
[35mnode2_1          |[0m %7|1586801646.476|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801646.476|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 47)
[35mnode2_1          |[0m %7|1586801646.541|OFFSET|rdkafka#consumer-1| [thrd:main]: Topic striot-queue [0]: stored offset 9, committed offset -1001: setting stored offset 9 for commit
[35mnode2_1          |[0m %7|1586801646.541|COMMIT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Committing offsets for 1 partition(s): cgrp auto commit timer
[35mnode2_1          |[0m %7|1586801646.541|OFFSET|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Enqueue OffsetCommitRequest(v7, 1/1 partition(s))): cgrp auto commit timer
[35mnode2_1          |[0m %7|1586801646.541|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent OffsetCommitRequest (v7, 131 bytes @ 0, CorrId 10)
[35mnode2_1          |[0m %7|1586801646.554|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received OffsetCommitResponse (v7, 32 bytes, CorrId 10, rtt 12.97ms)
[35mnode2_1          |[0m %7|1586801646.554|COMMIT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: OffsetCommit for 1 partition(s): cgrp auto commit timer: returned: Success
[35mnode2_1          |[0m %7|1586801646.578|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 47, rtt 101.81ms)
[35mnode2_1          |[0m %7|1586801646.578|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 9, LSO 9, Ver 2/2
[35mnode2_1          |[0m %7|1586801646.578|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 9 (v2)
[35mnode2_1          |[0m %7|1586801646.578|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801646.578|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 48)
[35mnode2_1          |[0m %7|1586801646.681|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 48, rtt 102.55ms)
[35mnode2_1          |[0m %7|1586801646.681|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 9, LSO 9, Ver 2/2
[35mnode2_1          |[0m %7|1586801646.681|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 9 (v2)
[35mnode2_1          |[0m %7|1586801646.681|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801646.681|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 49)
[35mnode2_1          |[0m %7|1586801646.784|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 49, rtt 102.73ms)
[35mnode2_1          |[0m %7|1586801646.784|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 9, LSO 9, Ver 2/2
[35mnode2_1          |[0m %7|1586801646.784|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 9 (v2)
[35mnode2_1          |[0m %7|1586801646.784|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801646.784|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 50)
[35mnode2_1          |[0m %7|1586801646.886|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 50, rtt 102.20ms)
[35mnode2_1          |[0m %7|1586801646.886|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 9, LSO 9, Ver 2/2
[35mnode2_1          |[0m %7|1586801646.886|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 9 (v2)
[35mnode2_1          |[0m %7|1586801646.886|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801646.886|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 51)
[35mnode2_1          |[0m %7|1586801646.989|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 51, rtt 102.70ms)
[35mnode2_1          |[0m %7|1586801646.989|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 9, LSO 9, Ver 2/2
[35mnode2_1          |[0m %7|1586801646.989|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 9 (v2)
[35mnode2_1          |[0m %7|1586801646.989|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801646.989|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 52)
[36;1mzookeeper_1      |[0m 2020-04-13 18:14:07,039 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@215] - Accepted socket connection from /192.168.64.4:43982
[36;1mzookeeper_1      |[0m 2020-04-13 18:14:07,042 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@949] - Client attempting to establish new session at /192.168.64.4:43982
[36;1mzookeeper_1      |[0m 2020-04-13 18:14:07,046 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@694] - Established session 0x100004dfc100001 with negotiated timeout 30000 for client /192.168.64.4:43982
[35mnode2_1          |[0m %7|1586801647.091|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 52, rtt 101.97ms)
[35mnode2_1          |[0m %7|1586801647.091|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 9, LSO 9, Ver 2/2
[35mnode2_1          |[0m %7|1586801647.091|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 9 (v2)
[35mnode2_1          |[0m %7|1586801647.091|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801647.091|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 53)
[35mnode2_1          |[0m %7|1586801647.193|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 53, rtt 101.98ms)
[35mnode2_1          |[0m %7|1586801647.193|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 9, LSO 9, Ver 2/2
[35mnode2_1          |[0m %7|1586801647.193|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 9 (v2)
[35mnode2_1          |[0m %7|1586801647.193|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801647.193|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 54)
[35mnode2_1          |[0m %7|1586801647.270|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 54, rtt 76.58ms)
[35mnode2_1          |[0m %7|1586801647.270|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 10, LSO 10, Ver 2/2
[35mnode2_1          |[0m %7|1586801647.270|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 9, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801647.270|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 10 (v2)
[35mnode2_1          |[0m %7|1586801647.270|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801647.270|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 55)
[34mnode3_1          |[0m Event {eventId = 8, manage = Nothing, time = Just 2020-04-13 18:14:05.264458955 UTC, value = Just "Incoming Message at Server: 8"}
[36;1mzookeeper_1      |[0m 2020-04-13 18:14:07,282 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@487] - Processed session termination for sessionid: 0x100004dfc100001
[36;1mzookeeper_1      |[0m 2020-04-13 18:14:07,287 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1056] - Closed socket connection for client /192.168.64.4:43982 which had sessionid 0x100004dfc100001
[35mnode2_1          |[0m %7|1586801647.373|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 55, rtt 102.92ms)
[35mnode2_1          |[0m %7|1586801647.373|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 10, LSO 10, Ver 2/2
[35mnode2_1          |[0m %7|1586801647.373|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 10 (v2)
[35mnode2_1          |[0m %7|1586801647.373|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801647.373|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 56)
[35mnode2_1          |[0m %7|1586801647.475|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 56, rtt 102.01ms)
[35mnode2_1          |[0m %7|1586801647.475|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 10, LSO 10, Ver 2/2
[35mnode2_1          |[0m %7|1586801647.475|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 10 (v2)
[35mnode2_1          |[0m %7|1586801647.475|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801647.475|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 57)
[35mnode2_1          |[0m %7|1586801647.578|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 57, rtt 103.49ms)
[35mnode2_1          |[0m %7|1586801647.578|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 10, LSO 10, Ver 2/2
[35mnode2_1          |[0m %7|1586801647.578|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 10 (v2)
[35mnode2_1          |[0m %7|1586801647.578|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801647.579|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 58)
[35mnode2_1          |[0m %7|1586801647.681|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 58, rtt 102.03ms)
[35mnode2_1          |[0m %7|1586801647.681|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 10, LSO 10, Ver 2/2
[35mnode2_1          |[0m %7|1586801647.681|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 10 (v2)
[35mnode2_1          |[0m %7|1586801647.681|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801647.681|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 59)
[35mnode2_1          |[0m %7|1586801647.783|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 59, rtt 102.00ms)
[35mnode2_1          |[0m %7|1586801647.783|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 10, LSO 10, Ver 2/2
[35mnode2_1          |[0m %7|1586801647.783|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 10 (v2)
[35mnode2_1          |[0m %7|1586801647.783|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801647.783|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 60)
[35mnode2_1          |[0m %7|1586801647.885|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 60, rtt 102.65ms)
[35mnode2_1          |[0m %7|1586801647.886|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 10, LSO 10, Ver 2/2
[35mnode2_1          |[0m %7|1586801647.886|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 10 (v2)
[35mnode2_1          |[0m %7|1586801647.886|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801647.886|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 61)
[35mnode2_1          |[0m %7|1586801647.988|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 61, rtt 102.81ms)
[35mnode2_1          |[0m %7|1586801647.988|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 10, LSO 10, Ver 2/2
[35mnode2_1          |[0m %7|1586801647.988|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 10 (v2)
[35mnode2_1          |[0m %7|1586801647.989|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801647.989|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 62)
[35mnode2_1          |[0m %7|1586801648.091|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 62, rtt 102.04ms)
[35mnode2_1          |[0m %7|1586801648.091|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 10, LSO 10, Ver 2/2
[35mnode2_1          |[0m %7|1586801648.091|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 10 (v2)
[35mnode2_1          |[0m %7|1586801648.091|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801648.091|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 63)
[35mnode2_1          |[0m %7|1586801648.193|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 63, rtt 102.57ms)
[35mnode2_1          |[0m %7|1586801648.193|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 10, LSO 10, Ver 2/2
[35mnode2_1          |[0m %7|1586801648.193|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 10 (v2)
[35mnode2_1          |[0m %7|1586801648.193|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801648.193|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 64)
[35mnode2_1          |[0m %7|1586801648.272|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 64, rtt 78.80ms)
[35mnode2_1          |[0m %7|1586801648.272|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 11, LSO 11, Ver 2/2
[35mnode2_1          |[0m %7|1586801648.272|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 10, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801648.272|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 11 (v2)
[35mnode2_1          |[0m %7|1586801648.272|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801648.272|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 65)
[34mnode3_1          |[0m Event {eventId = 9, manage = Nothing, time = Just 2020-04-13 18:14:06.265209878 UTC, value = Just "Incoming Message at Server: 9"}
[35mnode2_1          |[0m %7|1586801648.375|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 65, rtt 102.94ms)
[35mnode2_1          |[0m %7|1586801648.375|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 11, LSO 11, Ver 2/2
[35mnode2_1          |[0m %7|1586801648.375|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 11 (v2)
[35mnode2_1          |[0m %7|1586801648.375|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801648.376|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 66)
[35mnode2_1          |[0m %7|1586801648.478|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 66, rtt 102.64ms)
[35mnode2_1          |[0m %7|1586801648.478|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 11, LSO 11, Ver 2/2
[35mnode2_1          |[0m %7|1586801648.478|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 11 (v2)
[35mnode2_1          |[0m %7|1586801648.478|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801648.478|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 67)
[35mnode2_1          |[0m %7|1586801648.580|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 67, rtt 102.17ms)
[35mnode2_1          |[0m %7|1586801648.581|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 11, LSO 11, Ver 2/2
[35mnode2_1          |[0m %7|1586801648.581|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 11 (v2)
[35mnode2_1          |[0m %7|1586801648.581|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801648.581|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 68)
[35mnode2_1          |[0m %7|1586801648.622|HEARTBEAT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Heartbeat for group "striot_con_group" generation id 1
[35mnode2_1          |[0m %7|1586801648.622|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent HeartbeatRequest (v3, 91 bytes @ 0, CorrId 11)
[35mnode2_1          |[0m %7|1586801648.624|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received HeartbeatResponse (v3, 6 bytes, CorrId 11, rtt 1.51ms)
[35mnode2_1          |[0m %7|1586801648.683|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 68, rtt 101.88ms)
[35mnode2_1          |[0m %7|1586801648.683|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 11, LSO 11, Ver 2/2
[35mnode2_1          |[0m %7|1586801648.683|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 11 (v2)
[35mnode2_1          |[0m %7|1586801648.683|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801648.683|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 69)
[35mnode2_1          |[0m %7|1586801648.786|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 69, rtt 103.17ms)
[35mnode2_1          |[0m %7|1586801648.786|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 11, LSO 11, Ver 2/2
[35mnode2_1          |[0m %7|1586801648.786|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 11 (v2)
[35mnode2_1          |[0m %7|1586801648.786|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801648.786|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 70)
[35mnode2_1          |[0m %7|1586801648.889|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 70, rtt 102.89ms)
[35mnode2_1          |[0m %7|1586801648.889|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 11, LSO 11, Ver 2/2
[35mnode2_1          |[0m %7|1586801648.889|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 11 (v2)
[35mnode2_1          |[0m %7|1586801648.889|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801648.889|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 71)
[35mnode2_1          |[0m %7|1586801648.991|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 71, rtt 101.89ms)
[35mnode2_1          |[0m %7|1586801648.991|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 11, LSO 11, Ver 2/2
[35mnode2_1          |[0m %7|1586801648.991|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 11 (v2)
[35mnode2_1          |[0m %7|1586801648.991|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801648.991|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 72)
[35mnode2_1          |[0m %7|1586801649.094|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 72, rtt 102.90ms)
[35mnode2_1          |[0m %7|1586801649.094|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 11, LSO 11, Ver 2/2
[35mnode2_1          |[0m %7|1586801649.094|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 11 (v2)
[35mnode2_1          |[0m %7|1586801649.094|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801649.094|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 73)
[35mnode2_1          |[0m %7|1586801649.196|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 73, rtt 101.79ms)
[35mnode2_1          |[0m %7|1586801649.196|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 11, LSO 11, Ver 2/2
[35mnode2_1          |[0m %7|1586801649.196|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 11 (v2)
[35mnode2_1          |[0m %7|1586801649.196|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801649.196|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 74)
[35mnode2_1          |[0m %7|1586801649.274|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 74, rtt 77.63ms)
[35mnode2_1          |[0m %7|1586801649.274|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 12, LSO 12, Ver 2/2
[35mnode2_1          |[0m %7|1586801649.274|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 11, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801649.274|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 12 (v2)
[35mnode2_1          |[0m %7|1586801649.274|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801649.274|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 75)
[34mnode3_1          |[0m Event {eventId = 10, manage = Nothing, time = Just 2020-04-13 18:14:07.266333201 UTC, value = Just "Incoming Message at Server: 10"}
[35mnode2_1          |[0m %7|1586801649.376|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 75, rtt 102.17ms)
[35mnode2_1          |[0m %7|1586801649.376|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 12, LSO 12, Ver 2/2
[35mnode2_1          |[0m %7|1586801649.376|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 12 (v2)
[35mnode2_1          |[0m %7|1586801649.376|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801649.376|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 76)
[35mnode2_1          |[0m %7|1586801649.479|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 76, rtt 102.56ms)
[35mnode2_1          |[0m %7|1586801649.479|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 12, LSO 12, Ver 2/2
[35mnode2_1          |[0m %7|1586801649.479|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 12 (v2)
[35mnode2_1          |[0m %7|1586801649.479|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801649.479|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 77)
[35mnode2_1          |[0m %7|1586801649.582|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 77, rtt 103.24ms)
[35mnode2_1          |[0m %7|1586801649.582|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 12, LSO 12, Ver 2/2
[35mnode2_1          |[0m %7|1586801649.582|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 12 (v2)
[35mnode2_1          |[0m %7|1586801649.582|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801649.582|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 78)
[35mnode2_1          |[0m %7|1586801649.685|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 78, rtt 102.63ms)
[35mnode2_1          |[0m %7|1586801649.685|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 12, LSO 12, Ver 2/2
[35mnode2_1          |[0m %7|1586801649.685|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 12 (v2)
[35mnode2_1          |[0m %7|1586801649.685|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801649.685|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 79)
[35mnode2_1          |[0m %7|1586801649.788|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 79, rtt 102.79ms)
[35mnode2_1          |[0m %7|1586801649.788|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 12, LSO 12, Ver 2/2
[35mnode2_1          |[0m %7|1586801649.788|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 12 (v2)
[35mnode2_1          |[0m %7|1586801649.788|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801649.788|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 80)
[35mnode2_1          |[0m %7|1586801649.891|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 80, rtt 102.68ms)
[35mnode2_1          |[0m %7|1586801649.891|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 12, LSO 12, Ver 2/2
[35mnode2_1          |[0m %7|1586801649.891|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 12 (v2)
[35mnode2_1          |[0m %7|1586801649.891|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801649.891|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 81)
[35mnode2_1          |[0m %7|1586801649.992|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 81, rtt 101.64ms)
[35mnode2_1          |[0m %7|1586801649.993|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 12, LSO 12, Ver 2/2
[35mnode2_1          |[0m %7|1586801649.993|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 12 (v2)
[35mnode2_1          |[0m %7|1586801649.993|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801649.993|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 82)
[35mnode2_1          |[0m %7|1586801650.095|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 82, rtt 102.55ms)
[35mnode2_1          |[0m %7|1586801650.095|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 12, LSO 12, Ver 2/2
[35mnode2_1          |[0m %7|1586801650.095|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 12 (v2)
[35mnode2_1          |[0m %7|1586801650.095|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801650.095|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 83)
[35mnode2_1          |[0m %7|1586801650.198|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 83, rtt 102.45ms)
[35mnode2_1          |[0m %7|1586801650.198|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 12, LSO 12, Ver 2/2
[35mnode2_1          |[0m %7|1586801650.198|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 12 (v2)
[35mnode2_1          |[0m %7|1586801650.198|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801650.198|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 84)
[35mnode2_1          |[0m %7|1586801650.275|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 84, rtt 76.86ms)
[35mnode2_1          |[0m %7|1586801650.275|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 13, LSO 13, Ver 2/2
[35mnode2_1          |[0m %7|1586801650.275|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 12, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801650.275|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 13 (v2)
[35mnode2_1          |[0m %7|1586801650.275|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801650.275|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 85)
[34mnode3_1          |[0m Event {eventId = 11, manage = Nothing, time = Just 2020-04-13 18:14:08.267699179 UTC, value = Just "Incoming Message at Server: 11"}
[35mnode2_1          |[0m %7|1586801650.377|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 85, rtt 101.72ms)
[35mnode2_1          |[0m %7|1586801650.377|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 13, LSO 13, Ver 2/2
[35mnode2_1          |[0m %7|1586801650.377|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 13 (v2)
[35mnode2_1          |[0m %7|1586801650.377|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801650.377|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 86)
[35mnode2_1          |[0m %7|1586801650.479|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 86, rtt 101.72ms)
[35mnode2_1          |[0m %7|1586801650.479|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 13, LSO 13, Ver 2/2
[35mnode2_1          |[0m %7|1586801650.479|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 13 (v2)
[35mnode2_1          |[0m %7|1586801650.479|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801650.479|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 87)
[35mnode2_1          |[0m %7|1586801650.582|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 87, rtt 103.21ms)
[35mnode2_1          |[0m %7|1586801650.582|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 13, LSO 13, Ver 2/2
[35mnode2_1          |[0m %7|1586801650.582|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 13 (v2)
[35mnode2_1          |[0m %7|1586801650.582|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801650.582|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 88)
[35mnode2_1          |[0m %7|1586801650.685|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 88, rtt 102.61ms)
[35mnode2_1          |[0m %7|1586801650.685|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 13, LSO 13, Ver 2/2
[35mnode2_1          |[0m %7|1586801650.685|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 13 (v2)
[35mnode2_1          |[0m %7|1586801650.685|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801650.685|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 89)
[35mnode2_1          |[0m %7|1586801650.787|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 89, rtt 101.78ms)
[35mnode2_1          |[0m %7|1586801650.787|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 13, LSO 13, Ver 2/2
[35mnode2_1          |[0m %7|1586801650.787|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 13 (v2)
[35mnode2_1          |[0m %7|1586801650.787|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801650.787|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 90)
[35mnode2_1          |[0m %7|1586801650.888|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 90, rtt 101.65ms)
[35mnode2_1          |[0m %7|1586801650.888|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 13, LSO 13, Ver 2/2
[35mnode2_1          |[0m %7|1586801650.888|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 13 (v2)
[35mnode2_1          |[0m %7|1586801650.888|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801650.889|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 91)
[35mnode2_1          |[0m %7|1586801650.991|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 91, rtt 102.64ms)
[35mnode2_1          |[0m %7|1586801650.991|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 13, LSO 13, Ver 2/2
[35mnode2_1          |[0m %7|1586801650.991|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 13 (v2)
[35mnode2_1          |[0m %7|1586801650.991|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801650.991|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 92)
[35mnode2_1          |[0m %7|1586801651.094|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 92, rtt 102.50ms)
[35mnode2_1          |[0m %7|1586801651.094|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 13, LSO 13, Ver 2/2
[35mnode2_1          |[0m %7|1586801651.094|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 13 (v2)
[35mnode2_1          |[0m %7|1586801651.094|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801651.094|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 93)
[35mnode2_1          |[0m %7|1586801651.197|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 93, rtt 102.82ms)
[35mnode2_1          |[0m %7|1586801651.197|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 13, LSO 13, Ver 2/2
[35mnode2_1          |[0m %7|1586801651.197|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 13 (v2)
[35mnode2_1          |[0m %7|1586801651.197|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801651.197|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 94)
[35mnode2_1          |[0m %7|1586801651.276|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 94, rtt 79.44ms)
[35mnode2_1          |[0m %7|1586801651.276|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 14, LSO 14, Ver 2/2
[35mnode2_1          |[0m %7|1586801651.276|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 13, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801651.276|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 14 (v2)
[35mnode2_1          |[0m %7|1586801651.277|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801651.277|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 95)
[34mnode3_1          |[0m Event {eventId = 12, manage = Nothing, time = Just 2020-04-13 18:14:09.269083899 UTC, value = Just "Incoming Message at Server: 12"}
[35mnode2_1          |[0m %7|1586801651.379|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 95, rtt 102.26ms)
[35mnode2_1          |[0m %7|1586801651.379|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 14, LSO 14, Ver 2/2
[35mnode2_1          |[0m %7|1586801651.379|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 14 (v2)
[35mnode2_1          |[0m %7|1586801651.379|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801651.379|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 96)
[35mnode2_1          |[0m %7|1586801651.482|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 96, rtt 102.93ms)
[35mnode2_1          |[0m %7|1586801651.482|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 14, LSO 14, Ver 2/2
[35mnode2_1          |[0m %7|1586801651.482|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 14 (v2)
[35mnode2_1          |[0m %7|1586801651.482|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801651.482|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 97)
[35mnode2_1          |[0m %7|1586801651.541|OFFSET|rdkafka#consumer-1| [thrd:main]: Topic striot-queue [0]: stored offset 14, committed offset 9: setting stored offset 14 for commit
[35mnode2_1          |[0m %7|1586801651.541|COMMIT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Committing offsets for 1 partition(s): cgrp auto commit timer
[35mnode2_1          |[0m %7|1586801651.541|OFFSET|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Enqueue OffsetCommitRequest(v7, 1/1 partition(s))): cgrp auto commit timer
[35mnode2_1          |[0m %7|1586801651.541|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent OffsetCommitRequest (v7, 131 bytes @ 0, CorrId 12)
[35mnode2_1          |[0m %7|1586801651.545|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received OffsetCommitResponse (v7, 32 bytes, CorrId 12, rtt 3.37ms)
[35mnode2_1          |[0m %7|1586801651.545|COMMIT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: OffsetCommit for 1 partition(s): cgrp auto commit timer: returned: Success
[35mnode2_1          |[0m %7|1586801651.585|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 97, rtt 103.21ms)
[35mnode2_1          |[0m %7|1586801651.585|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 14, LSO 14, Ver 2/2
[35mnode2_1          |[0m %7|1586801651.585|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 14 (v2)
[35mnode2_1          |[0m %7|1586801651.585|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801651.585|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 98)
[35mnode2_1          |[0m %7|1586801651.623|HEARTBEAT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Heartbeat for group "striot_con_group" generation id 1
[35mnode2_1          |[0m %7|1586801651.623|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent HeartbeatRequest (v3, 91 bytes @ 0, CorrId 13)
[35mnode2_1          |[0m %7|1586801651.625|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received HeartbeatResponse (v3, 6 bytes, CorrId 13, rtt 1.63ms)
[35mnode2_1          |[0m %7|1586801651.688|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 98, rtt 102.42ms)
[35mnode2_1          |[0m %7|1586801651.688|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 14, LSO 14, Ver 2/2
[35mnode2_1          |[0m %7|1586801651.688|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 14 (v2)
[35mnode2_1          |[0m %7|1586801651.688|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801651.688|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 99)
[35mnode2_1          |[0m %7|1586801651.791|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 99, rtt 102.55ms)
[35mnode2_1          |[0m %7|1586801651.791|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 14, LSO 14, Ver 2/2
[35mnode2_1          |[0m %7|1586801651.791|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 14 (v2)
[35mnode2_1          |[0m %7|1586801651.791|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801651.791|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 100)
[35mnode2_1          |[0m %7|1586801651.892|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 100, rtt 101.81ms)
[35mnode2_1          |[0m %7|1586801651.893|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 14, LSO 14, Ver 2/2
[35mnode2_1          |[0m %7|1586801651.893|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 14 (v2)
[35mnode2_1          |[0m %7|1586801651.893|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801651.893|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 101)
[35mnode2_1          |[0m %7|1586801651.995|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 101, rtt 102.55ms)
[35mnode2_1          |[0m %7|1586801651.995|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 14, LSO 14, Ver 2/2
[35mnode2_1          |[0m %7|1586801651.995|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 14 (v2)
[35mnode2_1          |[0m %7|1586801651.995|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801651.995|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 102)
[35mnode2_1          |[0m %7|1586801652.098|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 102, rtt 102.59ms)
[35mnode2_1          |[0m %7|1586801652.098|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 14, LSO 14, Ver 2/2
[35mnode2_1          |[0m %7|1586801652.098|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 14 (v2)
[35mnode2_1          |[0m %7|1586801652.098|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801652.098|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 103)
[35mnode2_1          |[0m %7|1586801652.201|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 103, rtt 102.66ms)
[35mnode2_1          |[0m %7|1586801652.201|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 14, LSO 14, Ver 2/2
[35mnode2_1          |[0m %7|1586801652.201|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 14 (v2)
[35mnode2_1          |[0m %7|1586801652.201|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801652.201|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 104)
[35mnode2_1          |[0m %7|1586801652.277|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 104, rtt 76.59ms)
[35mnode2_1          |[0m %7|1586801652.277|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 15, LSO 15, Ver 2/2
[35mnode2_1          |[0m %7|1586801652.277|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 14, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801652.278|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 15 (v2)
[35mnode2_1          |[0m %7|1586801652.278|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801652.278|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 105)
[34mnode3_1          |[0m Event {eventId = 13, manage = Nothing, time = Just 2020-04-13 18:14:10.270453458 UTC, value = Just "Incoming Message at Server: 13"}
[35mnode2_1          |[0m %7|1586801652.380|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 105, rtt 102.33ms)
[35mnode2_1          |[0m %7|1586801652.380|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 15, LSO 15, Ver 2/2
[35mnode2_1          |[0m %7|1586801652.380|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 15 (v2)
[35mnode2_1          |[0m %7|1586801652.380|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801652.380|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 106)
[35mnode2_1          |[0m %7|1586801652.482|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 106, rtt 102.35ms)
[35mnode2_1          |[0m %7|1586801652.482|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 15, LSO 15, Ver 2/2
[35mnode2_1          |[0m %7|1586801652.482|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 15 (v2)
[35mnode2_1          |[0m %7|1586801652.483|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801652.483|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 107)
[35mnode2_1          |[0m %7|1586801652.585|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 107, rtt 102.64ms)
[35mnode2_1          |[0m %7|1586801652.585|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 15, LSO 15, Ver 2/2
[35mnode2_1          |[0m %7|1586801652.585|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 15 (v2)
[35mnode2_1          |[0m %7|1586801652.585|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801652.585|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 108)
[35mnode2_1          |[0m %7|1586801652.688|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 108, rtt 102.63ms)
[35mnode2_1          |[0m %7|1586801652.688|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 15, LSO 15, Ver 2/2
[35mnode2_1          |[0m %7|1586801652.688|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 15 (v2)
[35mnode2_1          |[0m %7|1586801652.688|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801652.688|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 109)
[35mnode2_1          |[0m %7|1586801652.791|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 109, rtt 102.59ms)
[35mnode2_1          |[0m %7|1586801652.791|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 15, LSO 15, Ver 2/2
[35mnode2_1          |[0m %7|1586801652.791|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 15 (v2)
[35mnode2_1          |[0m %7|1586801652.791|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801652.791|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 110)
[35mnode2_1          |[0m %7|1586801652.894|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 110, rtt 102.87ms)
[35mnode2_1          |[0m %7|1586801652.894|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 15, LSO 15, Ver 2/2
[35mnode2_1          |[0m %7|1586801652.894|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 15 (v2)
[35mnode2_1          |[0m %7|1586801652.894|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801652.894|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 111)
[35mnode2_1          |[0m %7|1586801652.997|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 111, rtt 102.67ms)
[35mnode2_1          |[0m %7|1586801652.997|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 15, LSO 15, Ver 2/2
[35mnode2_1          |[0m %7|1586801652.997|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 15 (v2)
[35mnode2_1          |[0m %7|1586801652.997|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801652.997|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 112)
[35mnode2_1          |[0m %7|1586801653.099|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 112, rtt 102.74ms)
[35mnode2_1          |[0m %7|1586801653.099|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 15, LSO 15, Ver 2/2
[35mnode2_1          |[0m %7|1586801653.099|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 15 (v2)
[35mnode2_1          |[0m %7|1586801653.100|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801653.100|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 113)
[35mnode2_1          |[0m %7|1586801653.202|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 113, rtt 102.69ms)
[35mnode2_1          |[0m %7|1586801653.202|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 15, LSO 15, Ver 2/2
[35mnode2_1          |[0m %7|1586801653.202|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 15 (v2)
[35mnode2_1          |[0m %7|1586801653.202|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801653.202|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 114)
[35mnode2_1          |[0m %7|1586801653.280|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 114, rtt 77.84ms)
[35mnode2_1          |[0m %7|1586801653.280|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 16, LSO 16, Ver 2/2
[35mnode2_1          |[0m %7|1586801653.280|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 15, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801653.280|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 16 (v2)
[35mnode2_1          |[0m %7|1586801653.280|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801653.280|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 115)
[34mnode3_1          |[0m Event {eventId = 14, manage = Nothing, time = Just 2020-04-13 18:14:11.271820024 UTC, value = Just "Incoming Message at Server: 14"}
[35mnode2_1          |[0m %7|1586801653.383|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 115, rtt 102.68ms)
[35mnode2_1          |[0m %7|1586801653.383|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 16, LSO 16, Ver 2/2
[35mnode2_1          |[0m %7|1586801653.383|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 16 (v2)
[35mnode2_1          |[0m %7|1586801653.383|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801653.383|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 116)
[35mnode2_1          |[0m %7|1586801653.486|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 116, rtt 102.93ms)
[35mnode2_1          |[0m %7|1586801653.486|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 16, LSO 16, Ver 2/2
[35mnode2_1          |[0m %7|1586801653.486|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 16 (v2)
[35mnode2_1          |[0m %7|1586801653.486|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801653.486|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 117)
[35mnode2_1          |[0m %7|1586801653.589|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 117, rtt 102.92ms)
[35mnode2_1          |[0m %7|1586801653.589|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 16, LSO 16, Ver 2/2
[35mnode2_1          |[0m %7|1586801653.589|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 16 (v2)
[35mnode2_1          |[0m %7|1586801653.589|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801653.589|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 118)
[35mnode2_1          |[0m %7|1586801653.692|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 118, rtt 102.71ms)
[35mnode2_1          |[0m %7|1586801653.692|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 16, LSO 16, Ver 2/2
[35mnode2_1          |[0m %7|1586801653.692|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 16 (v2)
[35mnode2_1          |[0m %7|1586801653.692|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801653.692|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 119)
[35mnode2_1          |[0m %7|1586801653.795|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 119, rtt 102.83ms)
[35mnode2_1          |[0m %7|1586801653.795|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 16, LSO 16, Ver 2/2
[35mnode2_1          |[0m %7|1586801653.795|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 16 (v2)
[35mnode2_1          |[0m %7|1586801653.795|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801653.795|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 120)
[35mnode2_1          |[0m %7|1586801653.898|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 120, rtt 102.46ms)
[35mnode2_1          |[0m %7|1586801653.898|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 16, LSO 16, Ver 2/2
[35mnode2_1          |[0m %7|1586801653.898|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 16 (v2)
[35mnode2_1          |[0m %7|1586801653.898|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801653.898|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 121)
[35mnode2_1          |[0m %7|1586801653.999|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 121, rtt 101.45ms)
[35mnode2_1          |[0m %7|1586801653.999|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 16, LSO 16, Ver 2/2
[35mnode2_1          |[0m %7|1586801653.999|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 16 (v2)
[35mnode2_1          |[0m %7|1586801653.999|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801653.999|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 122)
[35mnode2_1          |[0m %7|1586801654.102|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 122, rtt 102.51ms)
[35mnode2_1          |[0m %7|1586801654.102|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 16, LSO 16, Ver 2/2
[35mnode2_1          |[0m %7|1586801654.102|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 16 (v2)
[35mnode2_1          |[0m %7|1586801654.102|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801654.102|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 123)
[35mnode2_1          |[0m %7|1586801654.204|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 123, rtt 101.57ms)
[35mnode2_1          |[0m %7|1586801654.204|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 16, LSO 16, Ver 2/2
[35mnode2_1          |[0m %7|1586801654.204|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 16 (v2)
[35mnode2_1          |[0m %7|1586801654.204|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801654.204|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 124)
[35mnode2_1          |[0m %7|1586801654.281|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 124, rtt 77.10ms)
[35mnode2_1          |[0m %7|1586801654.281|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 17, LSO 17, Ver 2/2
[35mnode2_1          |[0m %7|1586801654.281|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 16, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801654.281|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 17 (v2)
[35mnode2_1          |[0m %7|1586801654.281|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801654.281|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 125)
[34mnode3_1          |[0m Event {eventId = 15, manage = Nothing, time = Just 2020-04-13 18:14:12.273175959 UTC, value = Just "Incoming Message at Server: 15"}
[35mnode2_1          |[0m %7|1586801654.383|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 125, rtt 102.36ms)
[35mnode2_1          |[0m %7|1586801654.383|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 17, LSO 17, Ver 2/2
[35mnode2_1          |[0m %7|1586801654.383|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 17 (v2)
[35mnode2_1          |[0m %7|1586801654.383|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801654.384|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 126)
[35mnode2_1          |[0m %7|1586801654.486|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 126, rtt 102.43ms)
[35mnode2_1          |[0m %7|1586801654.486|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 17, LSO 17, Ver 2/2
[35mnode2_1          |[0m %7|1586801654.486|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 17 (v2)
[35mnode2_1          |[0m %7|1586801654.486|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801654.486|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 127)
[35mnode2_1          |[0m %7|1586801654.589|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 127, rtt 103.15ms)
[35mnode2_1          |[0m %7|1586801654.589|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 17, LSO 17, Ver 2/2
[35mnode2_1          |[0m %7|1586801654.589|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 17 (v2)
[35mnode2_1          |[0m %7|1586801654.589|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801654.589|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 128)
[35mnode2_1          |[0m %7|1586801654.623|HEARTBEAT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Heartbeat for group "striot_con_group" generation id 1
[35mnode2_1          |[0m %7|1586801654.623|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent HeartbeatRequest (v3, 91 bytes @ 0, CorrId 14)
[35mnode2_1          |[0m %7|1586801654.625|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received HeartbeatResponse (v3, 6 bytes, CorrId 14, rtt 1.63ms)
[35mnode2_1          |[0m %7|1586801654.692|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 128, rtt 102.57ms)
[35mnode2_1          |[0m %7|1586801654.692|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 17, LSO 17, Ver 2/2
[35mnode2_1          |[0m %7|1586801654.692|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 17 (v2)
[35mnode2_1          |[0m %7|1586801654.692|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801654.692|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 129)
[35mnode2_1          |[0m %7|1586801654.795|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 129, rtt 102.56ms)
[35mnode2_1          |[0m %7|1586801654.795|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 17, LSO 17, Ver 2/2
[35mnode2_1          |[0m %7|1586801654.795|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 17 (v2)
[35mnode2_1          |[0m %7|1586801654.795|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801654.795|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 130)
[35mnode2_1          |[0m %7|1586801654.898|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 130, rtt 103.07ms)
[35mnode2_1          |[0m %7|1586801654.898|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 17, LSO 17, Ver 2/2
[35mnode2_1          |[0m %7|1586801654.898|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 17 (v2)
[35mnode2_1          |[0m %7|1586801654.898|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801654.898|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 131)
[35mnode2_1          |[0m %7|1586801655.000|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 131, rtt 102.38ms)
[35mnode2_1          |[0m %7|1586801655.000|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 17, LSO 17, Ver 2/2
[35mnode2_1          |[0m %7|1586801655.000|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 17 (v2)
[35mnode2_1          |[0m %7|1586801655.001|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801655.001|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 132)
[35mnode2_1          |[0m %7|1586801655.103|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 132, rtt 102.70ms)
[35mnode2_1          |[0m %7|1586801655.103|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 17, LSO 17, Ver 2/2
[35mnode2_1          |[0m %7|1586801655.103|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 17 (v2)
[35mnode2_1          |[0m %7|1586801655.103|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801655.103|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 133)
[35mnode2_1          |[0m %7|1586801655.205|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 133, rtt 101.92ms)
[35mnode2_1          |[0m %7|1586801655.205|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 17, LSO 17, Ver 2/2
[35mnode2_1          |[0m %7|1586801655.205|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 17 (v2)
[35mnode2_1          |[0m %7|1586801655.205|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801655.205|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 134)
[35mnode2_1          |[0m %7|1586801655.290|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 134, rtt 84.49ms)
[35mnode2_1          |[0m %7|1586801655.290|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 18, LSO 18, Ver 2/2
[35mnode2_1          |[0m %7|1586801655.290|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 17, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801655.290|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 18 (v2)
[35mnode2_1          |[0m %7|1586801655.290|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801655.290|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 135)
[34mnode3_1          |[0m Event {eventId = 16, manage = Nothing, time = Just 2020-04-13 18:14:13.274520534 UTC, value = Just "Incoming Message at Server: 16"}
[35mnode2_1          |[0m %7|1586801655.393|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 135, rtt 102.47ms)
[35mnode2_1          |[0m %7|1586801655.393|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 18, LSO 18, Ver 2/2
[35mnode2_1          |[0m %7|1586801655.393|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 18 (v2)
[35mnode2_1          |[0m %7|1586801655.393|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801655.393|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 136)
[35mnode2_1          |[0m %7|1586801655.496|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 136, rtt 102.82ms)
[35mnode2_1          |[0m %7|1586801655.496|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 18, LSO 18, Ver 2/2
[35mnode2_1          |[0m %7|1586801655.496|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 18 (v2)
[35mnode2_1          |[0m %7|1586801655.496|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801655.496|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 137)
[35mnode2_1          |[0m %7|1586801655.598|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 137, rtt 102.22ms)
[35mnode2_1          |[0m %7|1586801655.598|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 18, LSO 18, Ver 2/2
[35mnode2_1          |[0m %7|1586801655.598|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 18 (v2)
[35mnode2_1          |[0m %7|1586801655.598|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801655.598|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 138)
[35mnode2_1          |[0m %7|1586801655.699|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 138, rtt 101.15ms)
[35mnode2_1          |[0m %7|1586801655.699|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 18, LSO 18, Ver 2/2
[35mnode2_1          |[0m %7|1586801655.699|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 18 (v2)
[35mnode2_1          |[0m %7|1586801655.699|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801655.699|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 139)
[35mnode2_1          |[0m %7|1586801655.802|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 139, rtt 102.33ms)
[35mnode2_1          |[0m %7|1586801655.802|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 18, LSO 18, Ver 2/2
[35mnode2_1          |[0m %7|1586801655.802|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 18 (v2)
[35mnode2_1          |[0m %7|1586801655.802|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801655.802|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 140)
[35mnode2_1          |[0m %7|1586801655.903|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 140, rtt 101.26ms)
[35mnode2_1          |[0m %7|1586801655.903|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 18, LSO 18, Ver 2/2
[35mnode2_1          |[0m %7|1586801655.903|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 18 (v2)
[35mnode2_1          |[0m %7|1586801655.903|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801655.903|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 141)
[35mnode2_1          |[0m %7|1586801656.006|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 141, rtt 102.29ms)
[35mnode2_1          |[0m %7|1586801656.006|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 18, LSO 18, Ver 2/2
[35mnode2_1          |[0m %7|1586801656.006|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 18 (v2)
[35mnode2_1          |[0m %7|1586801656.006|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801656.006|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 142)
[35mnode2_1          |[0m %7|1586801656.108|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 142, rtt 102.21ms)
[35mnode2_1          |[0m %7|1586801656.108|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 18, LSO 18, Ver 2/2
[35mnode2_1          |[0m %7|1586801656.108|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 18 (v2)
[35mnode2_1          |[0m %7|1586801656.108|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801656.108|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 143)
[35mnode2_1          |[0m %7|1586801656.211|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 143, rtt 102.78ms)
[35mnode2_1          |[0m %7|1586801656.211|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 18, LSO 18, Ver 2/2
[35mnode2_1          |[0m %7|1586801656.211|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 18 (v2)
[35mnode2_1          |[0m %7|1586801656.211|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801656.211|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 144)
[35mnode2_1          |[0m %7|1586801656.283|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 144, rtt 71.57ms)
[35mnode2_1          |[0m %7|1586801656.283|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 19, LSO 19, Ver 2/2
[35mnode2_1          |[0m %7|1586801656.283|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 18, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801656.283|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 19 (v2)
[35mnode2_1          |[0m %7|1586801656.283|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801656.283|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 145)
[34mnode3_1          |[0m Event {eventId = 17, manage = Nothing, time = Just 2020-04-13 18:14:14.275878491 UTC, value = Just "Incoming Message at Server: 17"}
[35mnode2_1          |[0m %7|1586801656.384|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 145, rtt 101.57ms)
[35mnode2_1          |[0m %7|1586801656.384|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 19, LSO 19, Ver 2/2
[35mnode2_1          |[0m %7|1586801656.384|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 19 (v2)
[35mnode2_1          |[0m %7|1586801656.384|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801656.385|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 146)
[35mnode2_1          |[0m %7|1586801656.487|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 146, rtt 102.15ms)
[35mnode2_1          |[0m %7|1586801656.487|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 19, LSO 19, Ver 2/2
[35mnode2_1          |[0m %7|1586801656.487|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 19 (v2)
[35mnode2_1          |[0m %7|1586801656.487|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801656.487|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 147)
[35mnode2_1          |[0m %7|1586801656.541|OFFSET|rdkafka#consumer-1| [thrd:main]: Topic striot-queue [0]: stored offset 19, committed offset 14: setting stored offset 19 for commit
[35mnode2_1          |[0m %7|1586801656.541|COMMIT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Committing offsets for 1 partition(s): cgrp auto commit timer
[35mnode2_1          |[0m %7|1586801656.541|OFFSET|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Enqueue OffsetCommitRequest(v7, 1/1 partition(s))): cgrp auto commit timer
[35mnode2_1          |[0m %7|1586801656.542|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent OffsetCommitRequest (v7, 131 bytes @ 0, CorrId 15)
[35mnode2_1          |[0m %7|1586801656.544|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received OffsetCommitResponse (v7, 32 bytes, CorrId 15, rtt 2.85ms)
[35mnode2_1          |[0m %7|1586801656.544|COMMIT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: OffsetCommit for 1 partition(s): cgrp auto commit timer: returned: Success
[35mnode2_1          |[0m %7|1586801656.589|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 147, rtt 101.82ms)
[35mnode2_1          |[0m %7|1586801656.589|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 19, LSO 19, Ver 2/2
[35mnode2_1          |[0m %7|1586801656.589|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 19 (v2)
[35mnode2_1          |[0m %7|1586801656.589|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801656.589|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 148)
[35mnode2_1          |[0m %7|1586801656.691|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 148, rtt 102.01ms)
[35mnode2_1          |[0m %7|1586801656.691|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 19, LSO 19, Ver 2/2
[35mnode2_1          |[0m %7|1586801656.691|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 19 (v2)
[35mnode2_1          |[0m %7|1586801656.691|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801656.691|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 149)
[35mnode2_1          |[0m %7|1586801656.794|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 149, rtt 102.52ms)
[35mnode2_1          |[0m %7|1586801656.794|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 19, LSO 19, Ver 2/2
[35mnode2_1          |[0m %7|1586801656.794|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 19 (v2)
[35mnode2_1          |[0m %7|1586801656.794|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801656.794|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 150)
[35mnode2_1          |[0m %7|1586801656.896|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 150, rtt 102.63ms)
[35mnode2_1          |[0m %7|1586801656.896|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 19, LSO 19, Ver 2/2
[35mnode2_1          |[0m %7|1586801656.896|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 19 (v2)
[35mnode2_1          |[0m %7|1586801656.896|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801656.896|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 151)
[35mnode2_1          |[0m %7|1586801656.999|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 151, rtt 102.99ms)
[35mnode2_1          |[0m %7|1586801656.999|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 19, LSO 19, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.000|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 19 (v2)
[35mnode2_1          |[0m %7|1586801657.000|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.000|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 152)
[35mnode2_1          |[0m %7|1586801657.102|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 152, rtt 102.43ms)
[35mnode2_1          |[0m %7|1586801657.102|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 19, LSO 19, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.102|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 19 (v2)
[35mnode2_1          |[0m %7|1586801657.102|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.102|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 153)
[35mnode2_1          |[0m %7|1586801657.204|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 153, rtt 101.54ms)
[35mnode2_1          |[0m %7|1586801657.204|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 19, LSO 19, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.204|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 19 (v2)
[35mnode2_1          |[0m %7|1586801657.204|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.204|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 154)
[35mnode2_1          |[0m %7|1586801657.284|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 154, rtt 79.83ms)
[35mnode2_1          |[0m %7|1586801657.284|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 20, LSO 20, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.284|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 19, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801657.284|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 20 (v2)
[35mnode2_1          |[0m %7|1586801657.284|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.284|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 155)
[34mnode3_1          |[0m Event {eventId = 18, manage = Nothing, time = Just 2020-04-13 18:14:15.277178019 UTC, value = Just "Incoming Message at Server: 18"}
[35mnode2_1          |[0m %7|1586801657.386|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 155, rtt 102.04ms)
[35mnode2_1          |[0m %7|1586801657.386|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 20, LSO 20, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.386|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 20 (v2)
[35mnode2_1          |[0m %7|1586801657.386|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.386|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 156)
[35mnode2_1          |[0m %7|1586801657.488|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 156, rtt 102.12ms)
[35mnode2_1          |[0m %7|1586801657.488|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 20, LSO 20, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.488|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 20 (v2)
[35mnode2_1          |[0m %7|1586801657.488|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.488|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 157)
[35mnode2_1          |[0m %7|1586801657.591|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 157, rtt 102.61ms)
[35mnode2_1          |[0m %7|1586801657.591|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 20, LSO 20, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.591|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 20 (v2)
[35mnode2_1          |[0m %7|1586801657.591|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.591|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 158)
[35mnode2_1          |[0m %7|1586801657.623|HEARTBEAT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Heartbeat for group "striot_con_group" generation id 1
[35mnode2_1          |[0m %7|1586801657.623|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent HeartbeatRequest (v3, 91 bytes @ 0, CorrId 16)
[35mnode2_1          |[0m %7|1586801657.624|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received HeartbeatResponse (v3, 6 bytes, CorrId 16, rtt 1.17ms)
[35mnode2_1          |[0m %7|1586801657.693|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 158, rtt 102.08ms)
[35mnode2_1          |[0m %7|1586801657.693|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 20, LSO 20, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.693|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 20 (v2)
[35mnode2_1          |[0m %7|1586801657.693|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.693|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 159)
[35mnode2_1          |[0m %7|1586801657.794|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 159, rtt 101.09ms)
[35mnode2_1          |[0m %7|1586801657.794|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 20, LSO 20, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.794|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 20 (v2)
[35mnode2_1          |[0m %7|1586801657.794|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.794|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 160)
[35mnode2_1          |[0m %7|1586801657.896|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 160, rtt 102.01ms)
[35mnode2_1          |[0m %7|1586801657.896|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 20, LSO 20, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.896|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 20 (v2)
[35mnode2_1          |[0m %7|1586801657.896|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.897|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 161)
[35mnode2_1          |[0m %7|1586801657.999|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 161, rtt 102.16ms)
[35mnode2_1          |[0m %7|1586801657.999|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 20, LSO 20, Ver 2/2
[35mnode2_1          |[0m %7|1586801657.999|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 20 (v2)
[35mnode2_1          |[0m %7|1586801657.999|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801657.999|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 162)
[35mnode2_1          |[0m %7|1586801658.101|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 162, rtt 102.11ms)
[35mnode2_1          |[0m %7|1586801658.101|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 20, LSO 20, Ver 2/2
[35mnode2_1          |[0m %7|1586801658.101|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 20 (v2)
[35mnode2_1          |[0m %7|1586801658.101|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801658.101|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 163)
[35mnode2_1          |[0m %7|1586801658.203|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 163, rtt 102.35ms)
[35mnode2_1          |[0m %7|1586801658.203|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 20, LSO 20, Ver 2/2
[35mnode2_1          |[0m %7|1586801658.203|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 20 (v2)
[35mnode2_1          |[0m %7|1586801658.203|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801658.204|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 164)
[35mnode2_1          |[0m %7|1586801658.285|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 164, rtt 81.46ms)
[35mnode2_1          |[0m %7|1586801658.285|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 21, LSO 21, Ver 2/2
[35mnode2_1          |[0m %7|1586801658.285|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 20, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801658.285|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 21 (v2)
[35mnode2_1          |[0m %7|1586801658.285|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801658.285|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 165)
[34mnode3_1          |[0m Event {eventId = 19, manage = Nothing, time = Just 2020-04-13 18:14:16.278558799 UTC, value = Just "Incoming Message at Server: 19"}
[35mnode2_1          |[0m %7|1586801658.387|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 165, rtt 101.88ms)
[35mnode2_1          |[0m %7|1586801658.387|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 21, LSO 21, Ver 2/2
[35mnode2_1          |[0m %7|1586801658.387|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 21 (v2)
[35mnode2_1          |[0m %7|1586801658.387|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801658.387|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 166)
[35mnode2_1          |[0m %7|1586801658.489|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 166, rtt 102.18ms)
[35mnode2_1          |[0m %7|1586801658.489|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 21, LSO 21, Ver 2/2
[35mnode2_1          |[0m %7|1586801658.489|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 21 (v2)
[35mnode2_1          |[0m %7|1586801658.489|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801658.489|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 167)
[35mnode2_1          |[0m %7|1586801658.592|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 167, rtt 102.30ms)
[35mnode2_1          |[0m %7|1586801658.592|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 21, LSO 21, Ver 2/2
[35mnode2_1          |[0m %7|1586801658.592|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 21 (v2)
[35mnode2_1          |[0m %7|1586801658.592|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801658.592|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 168)
[35mnode2_1          |[0m %7|1586801658.694|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 168, rtt 102.11ms)
[35mnode2_1          |[0m %7|1586801658.694|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 21, LSO 21, Ver 2/2
[35mnode2_1          |[0m %7|1586801658.694|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 21 (v2)
[35mnode2_1          |[0m %7|1586801658.694|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801658.694|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 169)
[35mnode2_1          |[0m %7|1586801658.796|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 169, rtt 102.29ms)
[35mnode2_1          |[0m %7|1586801658.797|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 21, LSO 21, Ver 2/2
[35mnode2_1          |[0m %7|1586801658.797|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 21 (v2)
[35mnode2_1          |[0m %7|1586801658.797|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801658.797|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 170)
[35mnode2_1          |[0m %7|1586801658.899|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 170, rtt 102.01ms)
[35mnode2_1          |[0m %7|1586801658.899|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 21, LSO 21, Ver 2/2
[35mnode2_1          |[0m %7|1586801658.899|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 21 (v2)
[35mnode2_1          |[0m %7|1586801658.899|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801658.899|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 171)
[35mnode2_1          |[0m %7|1586801659.001|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 171, rtt 102.15ms)
[35mnode2_1          |[0m %7|1586801659.001|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 21, LSO 21, Ver 2/2
[35mnode2_1          |[0m %7|1586801659.001|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 21 (v2)
[35mnode2_1          |[0m %7|1586801659.001|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801659.001|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 172)
[35mnode2_1          |[0m %7|1586801659.102|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 172, rtt 101.01ms)
[35mnode2_1          |[0m %7|1586801659.102|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 21, LSO 21, Ver 2/2
[35mnode2_1          |[0m %7|1586801659.102|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 21 (v2)
[35mnode2_1          |[0m %7|1586801659.102|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801659.102|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 173)
[35mnode2_1          |[0m %7|1586801659.203|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 173, rtt 100.97ms)
[35mnode2_1          |[0m %7|1586801659.203|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 21, LSO 21, Ver 2/2
[35mnode2_1          |[0m %7|1586801659.203|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 21 (v2)
[35mnode2_1          |[0m %7|1586801659.203|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801659.203|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 174)
[35mnode2_1          |[0m %7|1586801659.286|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 174, rtt 83.00ms)
[35mnode2_1          |[0m %7|1586801659.286|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 22, LSO 22, Ver 2/2
[35mnode2_1          |[0m %7|1586801659.286|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 21, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801659.286|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 22 (v2)
[35mnode2_1          |[0m %7|1586801659.286|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801659.287|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 175)
[34mnode3_1          |[0m Event {eventId = 20, manage = Nothing, time = Just 2020-04-13 18:14:17.279900246 UTC, value = Just "Incoming Message at Server: 20"}
[35mnode2_1          |[0m %7|1586801659.389|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 175, rtt 102.02ms)
[35mnode2_1          |[0m %7|1586801659.389|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 22, LSO 22, Ver 2/2
[35mnode2_1          |[0m %7|1586801659.389|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 22 (v2)
[35mnode2_1          |[0m %7|1586801659.389|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801659.389|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 176)
[35mnode2_1          |[0m %7|1586801659.492|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 176, rtt 103.01ms)
[35mnode2_1          |[0m %7|1586801659.492|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 22, LSO 22, Ver 2/2
[35mnode2_1          |[0m %7|1586801659.492|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 22 (v2)
[35mnode2_1          |[0m %7|1586801659.492|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801659.492|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 177)
[35mnode2_1          |[0m %7|1586801659.595|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 177, rtt 102.89ms)
[35mnode2_1          |[0m %7|1586801659.595|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 22, LSO 22, Ver 2/2
[35mnode2_1          |[0m %7|1586801659.595|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 22 (v2)
[35mnode2_1          |[0m %7|1586801659.595|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801659.595|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 178)
[35mnode2_1          |[0m %7|1586801659.696|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 178, rtt 101.27ms)
[35mnode2_1          |[0m %7|1586801659.696|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 22, LSO 22, Ver 2/2
[35mnode2_1          |[0m %7|1586801659.696|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 22 (v2)
[35mnode2_1          |[0m %7|1586801659.696|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801659.696|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 179)
[35mnode2_1          |[0m %7|1586801659.799|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 179, rtt 102.30ms)
[35mnode2_1          |[0m %7|1586801659.799|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 22, LSO 22, Ver 2/2
[35mnode2_1          |[0m %7|1586801659.799|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 22 (v2)
[35mnode2_1          |[0m %7|1586801659.799|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801659.799|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 180)
[35mnode2_1          |[0m %7|1586801659.901|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 180, rtt 102.23ms)
[35mnode2_1          |[0m %7|1586801659.901|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 22, LSO 22, Ver 2/2
[35mnode2_1          |[0m %7|1586801659.901|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 22 (v2)
[35mnode2_1          |[0m %7|1586801659.901|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801659.901|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 181)
[35mnode2_1          |[0m %7|1586801660.003|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 181, rtt 102.32ms)
[35mnode2_1          |[0m %7|1586801660.004|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 22, LSO 22, Ver 2/2
[35mnode2_1          |[0m %7|1586801660.004|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 22 (v2)
[35mnode2_1          |[0m %7|1586801660.004|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801660.004|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 182)
[35mnode2_1          |[0m %7|1586801660.106|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 182, rtt 102.16ms)
[35mnode2_1          |[0m %7|1586801660.106|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 22, LSO 22, Ver 2/2
[35mnode2_1          |[0m %7|1586801660.106|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 22 (v2)
[35mnode2_1          |[0m %7|1586801660.106|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801660.106|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 183)
[35mnode2_1          |[0m %7|1586801660.208|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 183, rtt 102.01ms)
[35mnode2_1          |[0m %7|1586801660.208|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 22, LSO 22, Ver 2/2
[35mnode2_1          |[0m %7|1586801660.208|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 22 (v2)
[35mnode2_1          |[0m %7|1586801660.208|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801660.208|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 184)
[35mnode2_1          |[0m %7|1586801660.288|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 184, rtt 79.75ms)
[35mnode2_1          |[0m %7|1586801660.288|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 23, LSO 23, Ver 2/2
[35mnode2_1          |[0m %7|1586801660.288|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 22, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801660.288|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 23 (v2)
[35mnode2_1          |[0m %7|1586801660.288|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801660.288|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 185)
[34mnode3_1          |[0m Event {eventId = 21, manage = Nothing, time = Just 2020-04-13 18:14:18.281278742 UTC, value = Just "Incoming Message at Server: 21"}
[35mnode2_1          |[0m %7|1586801660.390|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 185, rtt 101.56ms)
[35mnode2_1          |[0m %7|1586801660.390|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 23, LSO 23, Ver 2/2
[35mnode2_1          |[0m %7|1586801660.390|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 23 (v2)
[35mnode2_1          |[0m %7|1586801660.390|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801660.390|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 186)
[35mnode2_1          |[0m %7|1586801660.492|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 186, rtt 102.29ms)
[35mnode2_1          |[0m %7|1586801660.492|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 23, LSO 23, Ver 2/2
[35mnode2_1          |[0m %7|1586801660.492|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 23 (v2)
[35mnode2_1          |[0m %7|1586801660.492|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801660.492|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 187)
[35mnode2_1          |[0m %7|1586801660.595|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 187, rtt 102.44ms)
[35mnode2_1          |[0m %7|1586801660.595|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 23, LSO 23, Ver 2/2
[35mnode2_1          |[0m %7|1586801660.595|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 23 (v2)
[35mnode2_1          |[0m %7|1586801660.595|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801660.595|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 188)
[35mnode2_1          |[0m %7|1586801660.623|HEARTBEAT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Heartbeat for group "striot_con_group" generation id 1
[35mnode2_1          |[0m %7|1586801660.623|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent HeartbeatRequest (v3, 91 bytes @ 0, CorrId 17)
[35mnode2_1          |[0m %7|1586801660.625|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received HeartbeatResponse (v3, 6 bytes, CorrId 17, rtt 1.69ms)
[35mnode2_1          |[0m %7|1586801660.697|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 188, rtt 102.25ms)
[35mnode2_1          |[0m %7|1586801660.697|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 23, LSO 23, Ver 2/2
[35mnode2_1          |[0m %7|1586801660.697|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 23 (v2)
[35mnode2_1          |[0m %7|1586801660.697|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801660.697|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 189)
[35mnode2_1          |[0m %7|1586801660.798|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 189, rtt 101.28ms)
[35mnode2_1          |[0m %7|1586801660.799|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 23, LSO 23, Ver 2/2
[35mnode2_1          |[0m %7|1586801660.799|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 23 (v2)
[35mnode2_1          |[0m %7|1586801660.799|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801660.799|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 190)
[35mnode2_1          |[0m %7|1586801660.901|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 190, rtt 102.07ms)
[35mnode2_1          |[0m %7|1586801660.901|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 23, LSO 23, Ver 2/2
[35mnode2_1          |[0m %7|1586801660.901|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 23 (v2)
[35mnode2_1          |[0m %7|1586801660.901|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801660.901|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 191)
[35mnode2_1          |[0m %7|1586801661.003|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 191, rtt 102.17ms)
[35mnode2_1          |[0m %7|1586801661.003|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 23, LSO 23, Ver 2/2
[35mnode2_1          |[0m %7|1586801661.003|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 23 (v2)
[35mnode2_1          |[0m %7|1586801661.003|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801661.003|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 192)
[35mnode2_1          |[0m %7|1586801661.104|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 192, rtt 100.90ms)
[35mnode2_1          |[0m %7|1586801661.104|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 23, LSO 23, Ver 2/2
[35mnode2_1          |[0m %7|1586801661.104|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 23 (v2)
[35mnode2_1          |[0m %7|1586801661.104|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801661.104|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 193)
[35mnode2_1          |[0m %7|1586801661.205|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 193, rtt 100.86ms)
[35mnode2_1          |[0m %7|1586801661.205|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 23, LSO 23, Ver 2/2
[35mnode2_1          |[0m %7|1586801661.205|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 23 (v2)
[35mnode2_1          |[0m %7|1586801661.205|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801661.205|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 194)
[35mnode2_1          |[0m %7|1586801661.289|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 325 bytes, CorrId 194, rtt 83.80ms)
[35mnode2_1          |[0m %7|1586801661.289|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 251, error "Success", MaxOffset 24, LSO 24, Ver 2/2
[35mnode2_1          |[0m %7|1586801661.289|CONSUME|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Enqueue 1 message(s) (181 bytes, 1 ops) on striot-queue [0] fetch queue (qlen 1, v2, last_offset 23, 0 ctrl msgs, uncompressed)
[35mnode2_1          |[0m %7|1586801661.289|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 24 (v2)
[35mnode2_1          |[0m %7|1586801661.289|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801661.289|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 195)
[34mnode3_1          |[0m Event {eventId = 22, manage = Nothing, time = Just 2020-04-13 18:14:19.282624407 UTC, value = Just "Incoming Message at Server: 22"}
[35mnode2_1          |[0m %7|1586801661.391|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 195, rtt 102.31ms)
[35mnode2_1          |[0m %7|1586801661.392|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 24, LSO 24, Ver 2/2
[35mnode2_1          |[0m %7|1586801661.392|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 24 (v2)
[35mnode2_1          |[0m %7|1586801661.392|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801661.392|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 196)
[35mnode2_1          |[0m %7|1586801661.494|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 196, rtt 101.97ms)
[35mnode2_1          |[0m %7|1586801661.494|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 24, LSO 24, Ver 2/2
[35mnode2_1          |[0m %7|1586801661.494|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 24 (v2)
[35mnode2_1          |[0m %7|1586801661.494|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801661.494|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 197)
[35mnode2_1          |[0m %7|1586801661.541|OFFSET|rdkafka#consumer-1| [thrd:main]: Topic striot-queue [0]: stored offset 24, committed offset 19: setting stored offset 24 for commit
[35mnode2_1          |[0m %7|1586801661.541|COMMIT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Committing offsets for 1 partition(s): cgrp auto commit timer
[35mnode2_1          |[0m %7|1586801661.541|OFFSET|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: Enqueue OffsetCommitRequest(v7, 1/1 partition(s))): cgrp auto commit timer
[35mnode2_1          |[0m %7|1586801661.542|SEND|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Sent OffsetCommitRequest (v7, 131 bytes @ 0, CorrId 18)
[35mnode2_1          |[0m %7|1586801661.544|RECV|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/1001: Received OffsetCommitResponse (v7, 32 bytes, CorrId 18, rtt 2.70ms)
[35mnode2_1          |[0m %7|1586801661.544|COMMIT|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/1001: OffsetCommit for 1 partition(s): cgrp auto commit timer: returned: Success
[35mnode2_1          |[0m %7|1586801661.596|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 197, rtt 101.77ms)
[35mnode2_1          |[0m %7|1586801661.596|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 24, LSO 24, Ver 2/2
[35mnode2_1          |[0m %7|1586801661.596|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 24 (v2)
[35mnode2_1          |[0m %7|1586801661.596|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801661.596|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 198)
[35mnode2_1          |[0m %7|1586801661.697|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 198, rtt 101.67ms)
[35mnode2_1          |[0m %7|1586801661.697|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 24, LSO 24, Ver 2/2
[35mnode2_1          |[0m %7|1586801661.697|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 24 (v2)
[35mnode2_1          |[0m %7|1586801661.697|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801661.697|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 199)
[35mnode2_1          |[0m %7|1586801661.800|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 199, rtt 102.44ms)
[35mnode2_1          |[0m %7|1586801661.800|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 24, LSO 24, Ver 2/2
[35mnode2_1          |[0m %7|1586801661.800|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 24 (v2)
[35mnode2_1          |[0m %7|1586801661.800|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801661.800|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 200)
[35mnode2_1          |[0m %7|1586801661.902|RECV|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Received FetchResponse (v11, 74 bytes, CorrId 200, rtt 101.91ms)
[35mnode2_1          |[0m %7|1586801661.902|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Topic striot-queue [0] MessageSet size 0, error "Success", MaxOffset 24, LSO 24, Ver 2/2
[35mnode2_1          |[0m %7|1586801661.902|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch topic striot-queue [0] at offset 24 (v2)
[35mnode2_1          |[0m %7|1586801661.902|FETCH|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Fetch 1/1/1 toppar(s)
[35mnode2_1          |[0m %7|1586801661.902|SEND|rdkafka#consumer-1| [thrd:kafka:9092/bootstrap]: kafka:9092/1001: Sent FetchRequest (v11, 102 bytes @ 0, CorrId 201)
Gracefully stopping... (press Ctrl+C again to force)
